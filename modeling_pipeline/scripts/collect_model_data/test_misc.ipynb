{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "todays_date = date.today().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parsl\n",
    "from parsl.app.app import python_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localParslConfig_threadpool(params):\n",
    " \n",
    "    import parsl\n",
    "    from parsl.executors import ThreadPoolExecutor\n",
    "    from parsl.config import Config\n",
    "\n",
    "    import os\n",
    "    workingdir = params['working_dir']\n",
    "    rundir = os.path.join(workingdir, 'runinfo')\n",
    "    parsl.clear()\n",
    "    local_tpex = Config(\n",
    "        executors=[\n",
    "            ThreadPoolExecutor(\n",
    "                label=\"tpex_Local\",\n",
    "                max_threads=8,\n",
    "                thread_name_prefix='tpex_run',\n",
    "                working_dir=workingdir,\n",
    "            )\n",
    "        ],\n",
    "        strategy=None,\n",
    "        run_dir=rundir\n",
    "    )\n",
    "\n",
    "    return(local_tpex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Currently on kawakami\n"
     ]
    }
   ],
   "source": [
    "enformer_predictions_path = \"/lus/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan/enformer_pipeline/enformer_predictions/kawakami/predictions_2022-12-20/kawakami\" #sys.argv[1] \n",
    "log_path = \"/lus/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan/enformer_pipeline/predictions_log/kawakami/predictions_log_2022-12-20\" #sys.argv[2]\n",
    "each_id = \"kawakami\" #sys.argv[3] # e.g. \"kawakami\" or \"cistrome\"\n",
    "agg_type = \"aggByMean\" #sys.argv[4]\n",
    "\n",
    "print(f'[INFO] Currently on {each_id}')\n",
    "\n",
    "upstream = list(range(0, 8))\n",
    "center = [8]\n",
    "downstream = list(range(9, 17))\n",
    "\n",
    "base_path = '/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan'\n",
    "# enformer_predictions_path = f'{base_path}/enformer_pipeline/enformer_predictions/{each_id}_reference/predictions_2022-12-11/{each_id}_FOXA1'\n",
    "# log_path = f'{base_path}/enformer_pipeline/predictions_log/predictions_log_2022-12-11'\n",
    "\n",
    "save_dir = f'{base_path}/modeling_pipeline/data/train-test-val/{each_id}/data_{todays_date}'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "TF = 'FOXA1'\n",
    "#data_names = ['aggByCenter']\n",
    "#data_names = ['aggByMean', 'aggByCenter', 'aggByUpstream', 'aggByDownstream', 'aggByUpstreamDownstream']\n",
    "\n",
    "logpath = f'{log_path}/{each_id}_{TF}_predictions_log.csv'\n",
    "log_data = pd.read_csv(logpath)\n",
    "log_data = log_data.drop_duplicates(subset=['motif']).iloc[1:20, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(lst, batch_n, len_lst = None):\n",
    "    \"\"\"\n",
    "    Given a list, this function yields batches of an unspecified size but the number of batches is equal to `batch_n`\n",
    "    E.g. generate_batch([0, 1, 2, 3, 4, 5, 6], batch_n=2) -> (0, 1, 2, 3), (4, 5, 6)\n",
    "    \n",
    "    Parameters:\n",
    "        lst: list\n",
    "        batch_n: int\n",
    "            Number of batches to return\n",
    "        len_lst: None or num (length of the input list)\n",
    "    Yields\n",
    "        `batch_n` batches of the list\n",
    "    \"\"\"\n",
    "    import math\n",
    "    # how many per batch\n",
    "    if len_lst is not None:\n",
    "        n_elems = math.ceil(len_lst/batch_n)\n",
    "    else:\n",
    "        n_elems = math.ceil(len(lst)/batch_n)\n",
    "        \n",
    "    for i in range(0, len(lst), n_elems):\n",
    "        yield lst[i:(i + n_elems)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(\n",
       "    app_cache=True, \n",
       "    checkpoint_files=None, \n",
       "    checkpoint_mode=None, \n",
       "    checkpoint_period=None, \n",
       "    executors=[ThreadPoolExecutor(\n",
       "        label='tpex_Local', \n",
       "        managed=True, \n",
       "        max_threads=8, \n",
       "        storage_access=None, \n",
       "        thread_name_prefix='tpex_run', \n",
       "        working_dir='/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan/modeling_pipeline'\n",
       "    )], \n",
       "    garbage_collect=True, \n",
       "    initialize_logging=True, \n",
       "    internal_tasks_max_threads=10, \n",
       "    max_idletime=120.0, \n",
       "    monitoring=None, \n",
       "    retries=0, \n",
       "    retry_handler=None, \n",
       "    run_dir='/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan/modeling_pipeline/runinfo', \n",
       "    strategy=None, \n",
       "    usage_tracking=False\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load parsl\n",
    "bpath = os.path.join(base_path, 'modeling_pipeline')\n",
    "localParslConfig_threadpool({'working_dir': bpath})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@python_app\n",
    "def collect_modeling_data_for_kawakami(each_id, log_data, predictions_path, TF, base_path, save_dir, agg_types, batch_num=None):\n",
    "\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    #import tqdm\n",
    "    # read in one of the files\n",
    "\n",
    "    try:\n",
    "        import utility_functions\n",
    "    except ModuleNotFoundError:\n",
    "        print(f'[ERROR] Utility_functions module not found.')\n",
    "\n",
    "    #exec(open(f'{base_path}/modeling_pipeline/scripts/collect_model_data/utility-functions.py').read(), globals(), globals())\n",
    "    # bpath = os.path.join(base_path, 'modeling_pipeline')\n",
    "    # localParslConfig_threadpool({'working_dir': bpath})\n",
    "\n",
    "    kawakami_predictions = {}\n",
    "\n",
    "    for dt in log_data.loc[log_data['sequence_type'] == 'ref', ].motif.values.tolist():\n",
    "        fle = f'{predictions_path}/{dt}_predictions.h5'\n",
    "        if os.path.isfile(fle):\n",
    "            with h5py.File(fle, 'r') as f:\n",
    "                filekey = list(f.keys())[0]\n",
    "                kawakami_predictions[dt] = np.vstack(list(f[filekey]))\n",
    "        else:\n",
    "            print(f'[ERROR] {dt} predictions file does not exist.')\n",
    "\n",
    "    print(f'[INFO] Finished collecting {len(kawakami_predictions)} predictions for {each_id}')\n",
    "\n",
    "    #dt_aggbycenter = agg_by_center(kawakami_predictions, center=8)\n",
    "    #data_list = [dt_aggbycenter]\n",
    "\n",
    "    data_dict = {}\n",
    "    for agg_type in agg_types:\n",
    "        if agg_type == 'aggByMean': data_dict[agg_type] = utility_functions.agg_by_mean(kawakami_predictions)\n",
    "        if agg_type == 'aggByCenter': data_dict[agg_type] = utility_functions.agg_by_center(kawakami_predictions)\n",
    "        if agg_type == 'aggByUpstream': data_dict[agg_type] = utility_functions.agg_by_mean(kawakami_predictions, use_bins=upstream)\n",
    "        if agg_type == 'aggByDownstream': data_dict[agg_type] = utility_functions.agg_by_mean(kawakami_predictions, use_bins=downstream)\n",
    "        if agg_type == 'aggByUpstreamDownstream': data_dict[agg_type] = utility_functions.agg_by_mean(kawakami_predictions, use_bins=upstream + downstream)\n",
    "\n",
    "    #test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream = agg_byall(kawakami_predictions)\n",
    "    #data_list = [test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream]\n",
    "\n",
    "    for i, agg_type in enumerate(data_dict):\n",
    "\n",
    "        #ty = pd.concat([pd.Series(list(kawakami_predictions.keys())), pd.DataFrame(dt)], axis=1)\n",
    "        ty = pd.DataFrame(data_dict[agg_type])\n",
    "        print(f'[INFO] Dimension of collected data is {ty.shape[0]} by {ty.shape[1]}')\n",
    "\n",
    "        column_names = ['id']\n",
    "        column_names.extend([f'f_{i}' for i in range(1, ty.shape[1])])\n",
    "\n",
    "        #print(len(column_names))\n",
    "        ty = ty.set_axis(column_names, axis=1, inplace=False)\n",
    "        print(ty.iloc[0:5, 0:5])\n",
    "\n",
    "        if batch_num is None:\n",
    "            ty.to_csv(path_or_buf=f'{save_dir}/{each_id}_{agg_type}_{TF}.csv.gz', index=False, compression='gzip')\n",
    "        else:\n",
    "            ty.to_csv(path_or_buf=f'{save_dir}/{each_id}_{agg_type}_{TF}_batch_{batch_num}.csv.gz', index=False, compression='gzip')\n",
    "            \n",
    "    print(f'[INFO] Finished saving data for {each_id}')\n",
    "\n",
    "    return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Must first load config",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m range_batches \u001b[39m=\u001b[39m generate_batch(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, log_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), batch_n\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m range_batch \u001b[39min\u001b[39;00m range_batches:\n\u001b[0;32m----> 6\u001b[0m     collected \u001b[39m=\u001b[39m collect_modeling_data_for_kawakami(each_id\u001b[39m=\u001b[39;49meach_id, log_data\u001b[39m=\u001b[39;49mlog_data\u001b[39m.\u001b[39;49miloc[\u001b[39mlist\u001b[39;49m(range_batch), ], predictions_path\u001b[39m=\u001b[39;49menformer_predictions_path, TF\u001b[39m=\u001b[39;49mTF, agg_types\u001b[39m=\u001b[39;49m[agg_type], base_path\u001b[39m=\u001b[39;49mbase_path, save_dir\u001b[39m=\u001b[39;49msave_dir, batch_num\u001b[39m=\u001b[39;49mcount)\n\u001b[1;32m      7\u001b[0m     app_futures\u001b[39m.\u001b[39mappend(collected)\n\u001b[1;32m      8\u001b[0m     count \u001b[39m=\u001b[39m count \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-tools/lib/python3.10/site-packages/parsl/app/python.py:68\u001b[0m, in \u001b[0;36mPythonApp.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m invocation_kwargs\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_flow_kernel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     dfk \u001b[39m=\u001b[39m DataFlowKernelLoader\u001b[39m.\u001b[39;49mdfk()\n\u001b[1;32m     69\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     dfk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_flow_kernel\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-tools/lib/python3.10/site-packages/parsl/dataflow/dflow.py:1348\u001b[0m, in \u001b[0;36mDataFlowKernelLoader.dfk\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[39m\"\"\"Return the currently-loaded DataFlowKernel.\"\"\"\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_dfk \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mMust first load config\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1349\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_dfk\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Must first load config"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "app_futures = []\n",
    "range_batches = generate_batch(range(0, log_data.shape[0] + 1), batch_n=3)\n",
    "for range_batch in range_batches:\n",
    "\n",
    "    collected = collect_modeling_data_for_kawakami(each_id=each_id, log_data=log_data.iloc[list(range_batch), ], predictions_path=enformer_predictions_path, TF=TF, agg_types=[agg_type], base_path=base_path, save_dir=save_dir, batch_num=count)\n",
    "    app_futures.append(collected)\n",
    "    count = count + 1\n",
    "\n",
    "app_execs = [r.result() for r in app_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished collecting 19 predictions for kawakami\n",
      "[INFO] Dimension of collected data is 19 by 5314\n",
      "                         id          f_1           f_2          f_3  \\\n",
      "0  chr3_192940730_192940738  0.052521303   0.059343856   0.08436744   \n",
      "1   chr12_38940385_38940393   0.05062969   0.042285476  0.031967606   \n",
      "2   chr10_31409432_31409440  0.004776896  0.0055661146  0.004850283   \n",
      "3  chr2_135230278_135230286  0.119184524    0.13753144  0.091157116   \n",
      "4   chr11_33763385_33763393   0.06338547    0.05521163  0.030234452   \n",
      "\n",
      "           f_4  \n",
      "0  0.077099375  \n",
      "1   0.08609707  \n",
      "2  0.008029616  \n",
      "3  0.104358084  \n",
      "4   0.06341503  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117367/2756105060.py:56: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  ty = ty.set_axis(column_names, axis=1, inplace=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished saving data for kawakami\n"
     ]
    }
   ],
   "source": [
    "collected = collect_modeling_data_for_kawakami(each_id=each_id, log_data=log_data, predictions_path=enformer_predictions_path, TF=TF, agg_types=[agg_type], base_path=base_path, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_modeling_data_for_kawakami(each_id, log_data, predictions_path, TF, data_names, base_path, save_dir):\n",
    "\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import tqdm\n",
    "    # read in one of the files\n",
    "\n",
    "    exec(open(f'{base_path}/modeling_pipeline/scripts/collect_model_data/utility-functions.py').read(), globals(), globals())\n",
    "\n",
    "    kawakami_predictions = {}\n",
    "\n",
    "    for dt in log_data.loc[log_data['sequence_type'] == 'ref', ].motif.values.tolist():\n",
    "        fle = f'{predictions_path}/{dt}_predictions.h5'\n",
    "        print(fle)\n",
    "        if os.path.isfile(fle):\n",
    "            with h5py.File(fle, 'r') as f:\n",
    "                filekey = list(f.keys())[0]\n",
    "                # should I select tracks? ; maybe not yet\n",
    "                kawakami_predictions[dt] = np.vstack(list(f[filekey]))\n",
    "        else:\n",
    "            print('File does not exist')\n",
    "\n",
    "    print(f'[INFO] Finished collecting {len(kawakami_predictions)} predictions for {each_id}')\n",
    "\n",
    "    dt_aggbycenter = agg_by_center(kawakami_predictions, center=8)\n",
    "    data_list = [dt_aggbycenter]\n",
    "\n",
    "    # test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream = agg_byall(freedman_predictions)\n",
    "    # data_list = [test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream]\n",
    "\n",
    "    for i, dt in enumerate(data_list):\n",
    "\n",
    "        ty = pd.concat([pd.Series(list(kawakami_predictions.keys())), pd.DataFrame(dt)], axis=1)\n",
    "\n",
    "        column_names = ['id', 'class']\n",
    "        column_names.extend([f'f_{i}' for i in range(1, ty.shape[1] - 1)])\n",
    "\n",
    "        ty = ty.set_axis(column_names, axis=1, copy=False)\n",
    "\n",
    "        ty.to_csv(path_or_buf=f'{save_dir}/{each_id}_{data_names[i]}_{TF}.csv.gz', index=False, compression='gzip')\n",
    "    print(f'[INFO] Finished saving data for {each_id}')\n",
    "\n",
    "    return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished collecting 0 predictions for kawakami\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m collected \u001b[39m=\u001b[39m collect_modeling_data_for_kawakami(each_id\u001b[39m=\u001b[39;49meach_id, log_data\u001b[39m=\u001b[39;49mlog_data, predictions_path\u001b[39m=\u001b[39;49menformer_predictions_path, TF\u001b[39m=\u001b[39;49mTF, data_names\u001b[39m=\u001b[39;49mdata_names, base_path\u001b[39m=\u001b[39;49mbase_path, save_dir\u001b[39m=\u001b[39;49msave_dir)\n\u001b[1;32m      3\u001b[0m \u001b[39m#print(collected)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[INFO] Status: \u001b[39m\u001b[39m{\u001b[39;00mcollected\u001b[39m}\u001b[39;00m\u001b[39m for \u001b[39m\u001b[39m{\u001b[39;00mlog_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m predictions for \u001b[39m\u001b[39m{\u001b[39;00meach_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [9], line 27\u001b[0m, in \u001b[0;36mcollect_modeling_data_for_kawakami\u001b[0;34m(each_id, log_data, predictions_path, TF, data_names, base_path, save_dir)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFile does not exist\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[INFO] Finished collecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(kawakami_predictions)\u001b[39m}\u001b[39;00m\u001b[39m predictions for \u001b[39m\u001b[39m{\u001b[39;00meach_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m dt_aggbycenter \u001b[39m=\u001b[39m agg_by_center(kawakami_predictions, center\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m data_list \u001b[39m=\u001b[39m [dt_aggbycenter]\n\u001b[1;32m     30\u001b[0m \u001b[39m# test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream = agg_byall(freedman_predictions)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# data_list = [test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream]\u001b[39;00m\n",
      "File \u001b[0;32m<string>:93\u001b[0m, in \u001b[0;36magg_by_center\u001b[0;34m(pred_tracks, center)\u001b[0m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-tools/lib/python3.10/site-packages/numpy/core/shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    281\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "collected = collect_modeling_data_for_kawakami(each_id=each_id, log_data=log_data, predictions_path=enformer_predictions_path, TF=TF, data_names=data_names, base_path=base_path, save_dir=save_dir)\n",
    "\n",
    "#print(collected)\n",
    "\n",
    "print(f'[INFO] Status: {collected} for {log_data.shape[0]} predictions for {each_id}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dcac1a71ccad94eb769456e80f90aa894f3068a5275ffeae8fac07a2d93c97a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
