{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "todays_date = date.today().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parsl\n",
    "from parsl.app.app import python_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localParslConfig_threadpool(params):\n",
    " \n",
    "    import parsl\n",
    "    from parsl.executors import ThreadPoolExecutor\n",
    "    from parsl.config import Config\n",
    "\n",
    "    import os\n",
    "    workingdir = params['working_dir']\n",
    "    rundir = os.path.join(workingdir, 'runinfo')\n",
    "    parsl.clear()\n",
    "    local_tpex = Config(\n",
    "        executors=[\n",
    "            ThreadPoolExecutor(\n",
    "                label=\"tpex_Local\",\n",
    "                max_threads=8,\n",
    "                thread_name_prefix='tpex_run',\n",
    "                working_dir=workingdir,\n",
    "            )\n",
    "        ],\n",
    "        strategy=None,\n",
    "        run_dir=rundir\n",
    "    )\n",
    "\n",
    "    return(local_tpex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Currently on kawakami\n"
     ]
    }
   ],
   "source": [
    "enformer_predictions_path = \"/lus/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan/enformer_pipeline/enformer_predictions/kawakami/predictions_2022-12-20/kawakami\" #sys.argv[1] \n",
    "log_path = \"/lus/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan/enformer_pipeline/predictions_log/kawakami/predictions_log_2022-12-20\" #sys.argv[2]\n",
    "each_id = \"kawakami\" #sys.argv[3] # e.g. \"kawakami\" or \"cistrome\"\n",
    "agg_type = \"aggByMean\" #sys.argv[4]\n",
    "\n",
    "print(f'[INFO] Currently on {each_id}')\n",
    "\n",
    "upstream = list(range(0, 8))\n",
    "center = [8]\n",
    "downstream = list(range(9, 17))\n",
    "\n",
    "base_path = '/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan'\n",
    "# enformer_predictions_path = f'{base_path}/enformer_pipeline/enformer_predictions/{each_id}_reference/predictions_2022-12-11/{each_id}_FOXA1'\n",
    "# log_path = f'{base_path}/enformer_pipeline/predictions_log/predictions_log_2022-12-11'\n",
    "\n",
    "save_dir = f'{base_path}/modeling_pipeline/data/train-test-val/{each_id}/data_{todays_date}'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "TF = 'FOXA1'\n",
    "#data_names = ['aggByCenter']\n",
    "#data_names = ['aggByMean', 'aggByCenter', 'aggByUpstream', 'aggByDownstream', 'aggByUpstreamDownstream']\n",
    "\n",
    "logpath = f'{log_path}/{each_id}_{TF}_predictions_log.csv'\n",
    "log_data = pd.read_csv(logpath)\n",
    "log_data = log_data.drop_duplicates(subset=['motif']).iloc[1:20, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(lst, batch_n, len_lst = None):\n",
    "    \"\"\"\n",
    "    Given a list, this function yields batches of an unspecified size but the number of batches is equal to `batch_n`\n",
    "    E.g. generate_batch([0, 1, 2, 3, 4, 5, 6], batch_n=2) -> (0, 1, 2, 3), (4, 5, 6)\n",
    "    \n",
    "    Parameters:\n",
    "        lst: list\n",
    "        batch_n: int\n",
    "            Number of batches to return\n",
    "        len_lst: None or num (length of the input list)\n",
    "    Yields\n",
    "        `batch_n` batches of the list\n",
    "    \"\"\"\n",
    "    import math\n",
    "    # how many per batch\n",
    "    if len_lst is not None:\n",
    "        n_elems = math.ceil(len_lst/batch_n)\n",
    "    else:\n",
    "        n_elems = math.ceil(len(lst)/batch_n)\n",
    "        \n",
    "    for i in range(0, len(lst), n_elems):\n",
    "        yield lst[i:(i + n_elems)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<parsl.dataflow.dflow.DataFlowKernel at 0x7fc34e49c190>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load parsl\n",
    "bpath = os.path.join(base_path, 'modeling_pipeline')\n",
    "parsl.load(localParslConfig_threadpool({'working_dir': bpath}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "@python_app\n",
    "def collect_modeling_data_for_kawakami(each_id, log_data, predictions_path, TF, base_path, save_dir, agg_types, batch_num=None):\n",
    "\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    #import tqdm\n",
    "    # read in one of the files\n",
    "\n",
    "    try:\n",
    "        import utility_functions\n",
    "    except ModuleNotFoundError:\n",
    "        print(f'[ERROR] Utility_functions module not found.')\n",
    "\n",
    "    #exec(open(f'{base_path}/modeling_pipeline/scripts/collect_model_data/utility-functions.py').read(), globals(), globals())\n",
    "    # bpath = os.path.join(base_path, 'modeling_pipeline')\n",
    "    # localParslConfig_threadpool({'working_dir': bpath})\n",
    "\n",
    "    kawakami_predictions = {}\n",
    "\n",
    "    for dt in log_data.loc[log_data['sequence_type'] == 'ref', ].motif.values.tolist():\n",
    "        fle = f'{predictions_path}/{dt}_predictions.h5'\n",
    "        if os.path.isfile(fle):\n",
    "            with h5py.File(fle, 'r') as f:\n",
    "                filekey = list(f.keys())[0]\n",
    "                kawakami_predictions[dt] = np.vstack(list(f[filekey]))\n",
    "        else:\n",
    "            print(f'[ERROR] {dt} predictions file does not exist.')\n",
    "\n",
    "    print(f'[INFO] Finished collecting {len(kawakami_predictions)} predictions for {each_id}')\n",
    "\n",
    "    #dt_aggbycenter = agg_by_center(kawakami_predictions, center=8)\n",
    "    #data_list = [dt_aggbycenter]\n",
    "\n",
    "    data_dict = {}\n",
    "    for agg_type in agg_types:\n",
    "        if agg_type == 'aggByMean': data_dict[agg_type] = utility_functions.agg_by_mean(kawakami_predictions)\n",
    "        if agg_type == 'aggByCenter': data_dict[agg_type] = utility_functions.agg_by_center(kawakami_predictions)\n",
    "        if agg_type == 'aggByUpstream': data_dict[agg_type] = utility_functions.agg_by_mean(kawakami_predictions, use_bins=upstream)\n",
    "        if agg_type == 'aggByDownstream': data_dict[agg_type] = utility_functions.agg_by_mean(kawakami_predictions, use_bins=downstream)\n",
    "        if agg_type == 'aggByUpstreamDownstream': data_dict[agg_type] = utility_functions.agg_by_mean(kawakami_predictions, use_bins=upstream + downstream)\n",
    "\n",
    "    #test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream = agg_byall(kawakami_predictions)\n",
    "    #data_list = [test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream]\n",
    "\n",
    "    for i, agg_type in enumerate(data_dict):\n",
    "\n",
    "        #ty = pd.concat([pd.Series(list(kawakami_predictions.keys())), pd.DataFrame(dt)], axis=1)\n",
    "        ty = pd.DataFrame(data_dict[agg_type])\n",
    "        print(f'[INFO] Dimension of collected data is {ty.shape[0]} by {ty.shape[1]}')\n",
    "\n",
    "        column_names = ['id']\n",
    "        column_names.extend([f'f_{i}' for i in range(1, ty.shape[1])])\n",
    "\n",
    "        #print(len(column_names))\n",
    "        ty = ty.set_axis(column_names, axis=1, inplace=False)\n",
    "        print(ty.iloc[0:5, 0:5])\n",
    "\n",
    "        if batch_num is None:\n",
    "            ty.to_csv(path_or_buf=f'{save_dir}/{each_id}_{agg_type}_{TF}.csv.gz', index=False, compression='gzip')\n",
    "        else:\n",
    "            ty.to_csv(path_or_buf=f'{save_dir}/{each_id}_{agg_type}_{TF}_batch_{batch_num}.csv.gz', index=False, compression='gzip')\n",
    "            \n",
    "    print(f'[INFO] Finished saving data for {each_id}')\n",
    "\n",
    "    return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_batches = generate_batch(range(0, log_data.shape[0]), batch_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      motif individual     status sequence_type\n",
      "1  chr3_192940730_192940738   kawakami  completed           ref\n",
      "2   chr12_38940385_38940393   kawakami  completed           ref\n",
      "3   chr10_31409432_31409440   kawakami  completed           ref\n",
      "4  chr2_135230278_135230286   kawakami  completed           ref\n",
      "5   chr11_33763385_33763393   kawakami  completed           ref\n",
      "6     chr20_9265818_9265826   kawakami  completed           ref\n",
      "7    chr4_42560188_42560196   kawakami  completed           ref\n",
      "                       motif individual     status sequence_type\n",
      "8    chr11_48899081_48899089   kawakami  completed           ref\n",
      "9    chr10_47536459_47536467   kawakami  completed           ref\n",
      "10  chr6_129623391_129623399   kawakami  completed           ref\n",
      "11   chr20_58329599_58329607   kawakami  completed           ref\n",
      "12   chr13_57434336_57434344   kawakami  completed           ref\n",
      "13   chr19_37202645_37202653   kawakami  completed           ref\n",
      "14    chr2_17741138_17741146   kawakami  completed           ref\n",
      "                       motif individual     status sequence_type\n",
      "15   chr10_98869401_98869409   kawakami  completed           ref\n",
      "16  chr9_125357974_125357982   kawakami  completed           ref\n",
      "17  chrX_127426104_127426112   kawakami  completed           ref\n",
      "18   chr19_20837597_20837605   kawakami  completed           ref\n",
      "19  chr6_136092701_136092709   kawakami  completed           ref\n"
     ]
    }
   ],
   "source": [
    "for r in range_batches:\n",
    "    print(log_data.iloc[list(r), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>motif</th>\n",
       "      <th>individual</th>\n",
       "      <th>status</th>\n",
       "      <th>sequence_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr3_192940730_192940738</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr12_38940385_38940393</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr10_31409432_31409440</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr2_135230278_135230286</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chr11_33763385_33763393</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chr20_9265818_9265826</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chr4_42560188_42560196</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>chr11_48899081_48899089</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>chr10_47536459_47536467</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>chr6_129623391_129623399</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>chr20_58329599_58329607</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>chr13_57434336_57434344</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>chr19_37202645_37202653</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>chr2_17741138_17741146</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>chr10_98869401_98869409</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>chr9_125357974_125357982</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>chrX_127426104_127426112</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>chr19_20837597_20837605</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>chr6_136092701_136092709</td>\n",
       "      <td>kawakami</td>\n",
       "      <td>completed</td>\n",
       "      <td>ref</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       motif individual     status sequence_type\n",
       "1   chr3_192940730_192940738   kawakami  completed           ref\n",
       "2    chr12_38940385_38940393   kawakami  completed           ref\n",
       "3    chr10_31409432_31409440   kawakami  completed           ref\n",
       "4   chr2_135230278_135230286   kawakami  completed           ref\n",
       "5    chr11_33763385_33763393   kawakami  completed           ref\n",
       "6      chr20_9265818_9265826   kawakami  completed           ref\n",
       "7     chr4_42560188_42560196   kawakami  completed           ref\n",
       "8    chr11_48899081_48899089   kawakami  completed           ref\n",
       "9    chr10_47536459_47536467   kawakami  completed           ref\n",
       "10  chr6_129623391_129623399   kawakami  completed           ref\n",
       "11   chr20_58329599_58329607   kawakami  completed           ref\n",
       "12   chr13_57434336_57434344   kawakami  completed           ref\n",
       "13   chr19_37202645_37202653   kawakami  completed           ref\n",
       "14    chr2_17741138_17741146   kawakami  completed           ref\n",
       "15   chr10_98869401_98869409   kawakami  completed           ref\n",
       "16  chr9_125357974_125357982   kawakami  completed           ref\n",
       "17  chrX_127426104_127426112   kawakami  completed           ref\n",
       "18   chr19_20837597_20837605   kawakami  completed           ref\n",
       "19  chr6_136092701_136092709   kawakami  completed           ref"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished collecting 5 predictions for kawakami\n",
      "[INFO] Dimension of collected data is 5 by 5314\n",
      "                         id          f_1          f_2          f_3  \\\n",
      "0   chr10_98869401_98869409   0.03649443   0.03414614   0.02135184   \n",
      "1  chr9_125357974_125357982  0.066400915  0.056020923   0.03805268   \n",
      "2  chrX_127426104_127426112  0.046020515  0.039052837  0.029655889   \n",
      "3   chr19_20837597_20837605   0.03973038  0.036300007   0.02595007   \n",
      "4  chr6_136092701_136092709  0.039942842  0.049277883   0.02926413   \n",
      "\n",
      "           f_4  \n",
      "0  0.102983005  \n",
      "1   0.08018745  \n",
      "2   0.07954953  \n",
      "3  0.061543964  \n",
      "4    0.0838695  \n",
      "[INFO] Finished collecting 7 predictions for kawakami\n",
      "[INFO] Finished collecting 7 predictions for kawakami\n",
      "[INFO] Dimension of collected data is 7 by 5314\n",
      "                         id          f_1          f_2          f_3  \\\n",
      "0   chr11_48899081_48899089   0.08941948   0.10191547   0.20742415   \n",
      "1   chr10_47536459_47536467   0.07756555   0.09292847   0.07055065   \n",
      "2  chr6_129623391_129623399  0.054028682  0.055472963  0.031370223   \n",
      "3   chr20_58329599_58329607   0.07396909  0.073623985   0.04743992   \n",
      "4   chr13_57434336_57434344  0.018997962  0.017526371  0.012235747   \n",
      "\n",
      "           f_4  \n",
      "0   0.07802225  \n",
      "1  0.101256624  \n",
      "2   0.09516511  \n",
      "3   0.09274977  \n",
      "4  0.031635664  \n",
      "[INFO] Dimension of collected data is 7 by 5314\n",
      "                         id          f_1           f_2          f_3  \\\n",
      "0  chr3_192940730_192940738  0.052521303   0.059343856   0.08436744   \n",
      "1   chr12_38940385_38940393   0.05062969   0.042285476  0.031967606   \n",
      "2   chr10_31409432_31409440  0.004776896  0.0055661146  0.004850283   \n",
      "3  chr2_135230278_135230286  0.119184524    0.13753144  0.091157116   \n",
      "4   chr11_33763385_33763393   0.06338547    0.05521163  0.030234452   \n",
      "\n",
      "           f_4  \n",
      "0  0.077099375  \n",
      "1   0.08609707  \n",
      "2  0.008029616  \n",
      "3  0.104358084  \n",
      "4   0.06341503  \n",
      "[INFO] Finished saving data for kawakami\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117367/4153071015.py:57: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  ty = ty.set_axis(column_names, axis=1, inplace=False)\n",
      "/tmp/ipykernel_117367/4153071015.py:57: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  ty = ty.set_axis(column_names, axis=1, inplace=False)\n",
      "/tmp/ipykernel_117367/4153071015.py:57: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  ty = ty.set_axis(column_names, axis=1, inplace=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished saving data for kawakami\n",
      "[INFO] Finished saving data for kawakami\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "app_futures = []\n",
    "for range_batch in range_batches:\n",
    "\n",
    "    collected = collect_modeling_data_for_kawakami(each_id=each_id, log_data=log_data.iloc[list(range_batch), ], predictions_path=enformer_predictions_path, TF=TF, agg_types=[agg_type], base_path=base_path, save_dir=save_dir, batch_num=count)\n",
    "    app_futures.append(collected)\n",
    "    count = count + 1\n",
    "\n",
    "app_execs = [r.result() for r in app_futures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished collecting 19 predictions for kawakami\n",
      "[INFO] Dimension of collected data is 19 by 5314\n",
      "                         id          f_1           f_2          f_3  \\\n",
      "0  chr3_192940730_192940738  0.052521303   0.059343856   0.08436744   \n",
      "1   chr12_38940385_38940393   0.05062969   0.042285476  0.031967606   \n",
      "2   chr10_31409432_31409440  0.004776896  0.0055661146  0.004850283   \n",
      "3  chr2_135230278_135230286  0.119184524    0.13753144  0.091157116   \n",
      "4   chr11_33763385_33763393   0.06338547    0.05521163  0.030234452   \n",
      "\n",
      "           f_4  \n",
      "0  0.077099375  \n",
      "1   0.08609707  \n",
      "2  0.008029616  \n",
      "3  0.104358084  \n",
      "4   0.06341503  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117367/2756105060.py:56: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  ty = ty.set_axis(column_names, axis=1, inplace=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished saving data for kawakami\n"
     ]
    }
   ],
   "source": [
    "collected = collect_modeling_data_for_kawakami(each_id=each_id, log_data=log_data, predictions_path=enformer_predictions_path, TF=TF, agg_types=[agg_type], base_path=base_path, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_modeling_data_for_kawakami(each_id, log_data, predictions_path, TF, data_names, base_path, save_dir):\n",
    "\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import tqdm\n",
    "    # read in one of the files\n",
    "\n",
    "    exec(open(f'{base_path}/modeling_pipeline/scripts/collect_model_data/utility-functions.py').read(), globals(), globals())\n",
    "\n",
    "    kawakami_predictions = {}\n",
    "\n",
    "    for dt in log_data.loc[log_data['sequence_type'] == 'ref', ].motif.values.tolist():\n",
    "        fle = f'{predictions_path}/{dt}_predictions.h5'\n",
    "        print(fle)\n",
    "        if os.path.isfile(fle):\n",
    "            with h5py.File(fle, 'r') as f:\n",
    "                filekey = list(f.keys())[0]\n",
    "                # should I select tracks? ; maybe not yet\n",
    "                kawakami_predictions[dt] = np.vstack(list(f[filekey]))\n",
    "        else:\n",
    "            print('File does not exist')\n",
    "\n",
    "    print(f'[INFO] Finished collecting {len(kawakami_predictions)} predictions for {each_id}')\n",
    "\n",
    "    dt_aggbycenter = agg_by_center(kawakami_predictions, center=8)\n",
    "    data_list = [dt_aggbycenter]\n",
    "\n",
    "    # test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream = agg_byall(freedman_predictions)\n",
    "    # data_list = [test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream]\n",
    "\n",
    "    for i, dt in enumerate(data_list):\n",
    "\n",
    "        ty = pd.concat([pd.Series(list(kawakami_predictions.keys())), pd.DataFrame(dt)], axis=1)\n",
    "\n",
    "        column_names = ['id', 'class']\n",
    "        column_names.extend([f'f_{i}' for i in range(1, ty.shape[1] - 1)])\n",
    "\n",
    "        ty = ty.set_axis(column_names, axis=1, copy=False)\n",
    "\n",
    "        ty.to_csv(path_or_buf=f'{save_dir}/{each_id}_{data_names[i]}_{TF}.csv.gz', index=False, compression='gzip')\n",
    "    print(f'[INFO] Finished saving data for {each_id}')\n",
    "\n",
    "    return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Finished collecting 0 predictions for kawakami\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m collected \u001b[39m=\u001b[39m collect_modeling_data_for_kawakami(each_id\u001b[39m=\u001b[39;49meach_id, log_data\u001b[39m=\u001b[39;49mlog_data, predictions_path\u001b[39m=\u001b[39;49menformer_predictions_path, TF\u001b[39m=\u001b[39;49mTF, data_names\u001b[39m=\u001b[39;49mdata_names, base_path\u001b[39m=\u001b[39;49mbase_path, save_dir\u001b[39m=\u001b[39;49msave_dir)\n\u001b[1;32m      3\u001b[0m \u001b[39m#print(collected)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[INFO] Status: \u001b[39m\u001b[39m{\u001b[39;00mcollected\u001b[39m}\u001b[39;00m\u001b[39m for \u001b[39m\u001b[39m{\u001b[39;00mlog_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m predictions for \u001b[39m\u001b[39m{\u001b[39;00meach_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [9], line 27\u001b[0m, in \u001b[0;36mcollect_modeling_data_for_kawakami\u001b[0;34m(each_id, log_data, predictions_path, TF, data_names, base_path, save_dir)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFile does not exist\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[INFO] Finished collecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(kawakami_predictions)\u001b[39m}\u001b[39;00m\u001b[39m predictions for \u001b[39m\u001b[39m{\u001b[39;00meach_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m dt_aggbycenter \u001b[39m=\u001b[39m agg_by_center(kawakami_predictions, center\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m)\n\u001b[1;32m     28\u001b[0m data_list \u001b[39m=\u001b[39m [dt_aggbycenter]\n\u001b[1;32m     30\u001b[0m \u001b[39m# test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream = agg_byall(freedman_predictions)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# data_list = [test_aggbymean, test_aggbycenter, test_aggbymean_upstream, test_aggbymean_downstream, test_aggbymean_upstream_downstream]\u001b[39;00m\n",
      "File \u001b[0;32m<string>:93\u001b[0m, in \u001b[0;36magg_by_center\u001b[0;34m(pred_tracks, center)\u001b[0m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-tools/lib/python3.10/site-packages/numpy/core/shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    281\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "collected = collect_modeling_data_for_kawakami(each_id=each_id, log_data=log_data, predictions_path=enformer_predictions_path, TF=TF, data_names=data_names, base_path=base_path, save_dir=save_dir)\n",
    "\n",
    "#print(collected)\n",
    "\n",
    "print(f'[INFO] Status: {collected} for {log_data.shape[0]} predictions for {each_id}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 with Fil",
   "language": "python",
   "name": "filprofile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:36:39) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dcac1a71ccad94eb769456e80f90aa894f3068a5275ffeae8fac07a2d93c97a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
