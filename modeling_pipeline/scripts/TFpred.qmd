---
title: "Build models on Kawakami data: elastic net and xgboost"
author: "Temi"
date: 'Wednesday Oct 26 2022'
format: 
  pdf: 
    toc: true
    number-sections: true
    code-line-numbers: true
---

```{r}
rm(list=ls())

library(glue)
library(reticulate)
library(R.utils)
library(data.table)
library(glmnet)
library(doMC)
library(ROCR)
library(Matrix)
library(reshape2)
library(tidyverse)
library(foreach)
library(doParallel)

```

```{r}
project_dir <- '/lus/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan/modeling_pipeline'
setwd(project_dir)

TF <- 'FOXA1'
id <- 'kawakami'
kawakami_data_dir <- glue('{project_dir}/data/train-test-val/kawakami/data_2022-12-16')
```

```{r}
# using the aggByCenter 
kawakami_center_dt <- data.table::fread(glue('{kawakami_data_dir}/{id}_aggByCenter_{TF}.csv.gz'))
dim(kawakami_center_dt) ; kawakami_center_dt[1:5, 1:5]
```

Ground truth (or class)

```{r}
ground_truth_path <- Sys.glob(paste0(project_dir, '/../impact_pipeline/motif_intervals/kawakami/intervals_2022-12-15/ground_truth/', id, '_', TF, '_*.txt'))
ground_truth_path
```

```{r}
ground_truth <- data.table::fread(ground_truth_path)
head(ground_truth)
```

```{r}
gt <- ground_truth[ground_truth$V1 %in% kawakami_center_dt$id, ]
dim(gt) ; length(unique(gt$V1))
```

```{r}
find_duplicates_in_dataframe <- function(dt, col, return_dups=TRUE){
  n_occur <- data.frame(table(dt[[col]]))

  if(return_dups == TRUE){
    return(dt[dt[[col]] %in% n_occur$Var1[n_occur$Freq > 1],])
  } else {
    return(n_occur[n_occur$Freq > 1,])
  }
}
```

```{r}
find_duplicates_in_dataframe(gt, col='V1')
```
Keep first occurrence of duplicates
```{r}
gt_dedup <- gt[!duplicated(gt[['V1']]),]
dim(gt_dedup)
```

Merge `gt_dedup` with the data
```{r}
new_dt <- merge(gt_dedup, kawakami_center_dt, by.x='V1', by.y='id')
new_dt[1:5, 1:5]
```

```{r}
#new_dt <- new_dt[, -c('class')]
colnames(new_dt)[1:3] <- c('region', 'class', 'binding_counts')
new_dt[1:5, 1:5] ; dim(new_dt)
```

Split into 80-20
```{r}
set.seed(2022)
tr_size <- ceiling(nrow(new_dt) * 1)
tr_indices <- sample(1:nrow(new_dt), tr_size)
```

```{r}
train <- new_dt[tr_indices, ]
test <- new_dt[-tr_indices, ]
```

```{r}
train[1:5, 1:5]
```

Save the data
```{r}
save_dir <- paste0(project_dir, '/data/enet_data/data_', Sys.Date())
if(!dir.exists(save_dir)){
  dir.create(save_dir)
}
```
```{r}
write.csv(x=train, file=gzfile(glue('{save_dir}/train_enet_{Sys.Date()}.csv.gz')), quote=F, row.names=F)
#write.csv(x=test, file=gzfile(paste0(save_dir, '/test_enet.csv.gz')))
```

```{r}
temp_dt <- data.table::fread(glue('{save_dir}/train_enet_{Sys.Date()}.csv.gz'))
temp_dt[1:5, 1:5] ; temp_dt |> dim()
```
Good!



```{r}
# split the data

X_train <- kawakami_center_dt[, -c(1,2)] |> as.matrix()
y_train <- kawakami_center_dt[, c(1,2)] |> as.data.frame()
```

```{r}
# check the distribution
y_train$class |> table()
```

### Utility functions
```{r}
source(glue('{project_dir}/scripts/utility-functions.R'))
```

```{r}
dt_train <- data.table::fread(paste0(project_dir, '/data/enet_data/data_2022-12-13/balanced_enet.csv.gz'))
X_train <- dt_train[, -c(1,2,3)] |> as.matrix()
y_train <- dt_train[, c(1,2,3)] |> as.data.frame()
```

```{r}
log10(freedman_linear_truth$num + 1)/sum(log10(freedman_linear_truth$num + 1))
```

```{r}
vbc <- log10(y_test$binding_counts + 1)
nbc <- vbc/sum(vbc)
```

```{r}
vbc <- y_test$binding_counts
nbc <- (vbc - min(vbc))/(max(vbc) - min(vbc))
```

```{r}
nbc[nbc > 0.9]
```





\newpage
### Modelling: elastic net
I have moved this section to a stand-alone script that trains and saves the model. It takes some time to build the model and I needed some compute power.
```{r}
# path to the data
training_data <- paste0(project_dir, '/data/enet_data/data_2022-12-16/train_enet_2022-12-16.csv.gz')
training_data ; file.exists(training_data)
```

```{r}
mtinfo <- 'with_mixing_parameters'
```

```{r}
pbs_script <- glue('qsub -v data_file={training_data},metainfo={mtinfo} {project_dir}/scripts/enet/train_enet_model_pbs.sh')
pbs_script #; file.exists(pbs_script)
```

```{r}
system(pbs_script)
```

```{r}
system("qstat -u temi")
```

Read in the model(s)
```{r}
enet_binary_model <- readRDS(paste0(project_dir, '/models/enet_models/kawakami_center_binary_matched_2022-12-16.rds'))
enet_binary_model
```

```{r}
enet_linear_model <- readRDS(paste0(project_dir, '/models/enet_models/kawakami_center_linear_balanced_2022-12-14.rds'))
enet_linear_model
```

```{r}
enet_binary_old_model <- readRDS(paste0(project_dir, '/models/enet_models/kawakami_center_binary_old_2022-12-14.rds'))
enet_binary_old_model
```




Test on the training data itself
```{r}
#test_data <- data.table::fread(paste0(project_dir, '/data/enet_data/data_', Sys.Date(), '/balanced_enet.csv.gz'))
test_data <- data.table::fread(training_data)
X_test <- test_data[, -c(1,2,3)] |> as.matrix()
y_test <- test_data[, c(1,2,3)] |> as.data.frame()

vbc <- y_test$binding_counts
y_test$nbc <- (vbc - min(vbc))/(max(vbc) - min(vbc))

X_test |> dim() ; X_test[1:5, 1:5] ; y_test |> head()
```

```{r}
tr_data <- data.table::fread('/lus/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan/modeling_pipeline/data/train-test-val/kawakami/data_2022-12-12/kawakami_aggByCenter_FOXA1_old.csv.gz')
```

```{r}
tr_data[1:5, 1:5]
```

### Some fancy plots
```{r}
a <- predict(enet_binary_old_model, tr_data[, -1] |> as.matrix(), type='response') #|> boxplot()
b <- data.frame(probabilities=a[,1], true_class=tr_data[, 1])
```

```{r}
boxplot(probabilities ~ class, data=b, col=c('aquamarine', 'darkorange'))
mtext('Boxplot of distribution of probabilities', line=1, cex=1.5)
```

```{r}
denx <- density(b$probabilities[b$class == 0])
deny <- density(b$probabilities[b$class == 1])


#den_unbound <- density(denx)
plot(denx, ylim = c(0, max(c(denx$y, deny$y))),
     xlim = c(min(c(denx$x, deny$x)),
              max(c(denx$x, deny$x))),
              main='Density plot of probabilities', xlab='probabilities')
lines(deny)

polygon(denx, col = adjustcolor("aquamarine", alpha.f=0.5) )
polygon(deny, col = adjustcolor("darkorange", alpha.f=0.5))
```

```{r}
predicted <- predict(enet_binary_model, X_test, type='class')
prediction_table <- cbind(observed=y_test$class, predicted=as.numeric(predicted)) |> as.data.frame()
prediction_table[1:5, ] ; with(prediction_table, table(observed, predicted))
```

```{r}
predicted <- predict(enet_binary_old_model, X_test, type='class')
prediction_table <- cbind(observed=y_test$class, predicted=as.numeric(predicted)) |> as.data.frame()
prediction_table[1:5, ] ; with(prediction_table, table(observed, predicted))
```

### Some fancy plots
```{r}
a <- predict(enet_binary_model, X_test, type='response') #|> boxplot()
b <- cbind(probabilities=a[,1], true_class=y_test$class) |> as.data.frame()
```

```{r}
boxplot(probabilities ~ true_class, data=b, col=c('aquamarine', 'darkorange'))
mtext('Boxplot of distribution of probabilities', line=1, cex=1.5)
```

```{r}
denx <- density(b$probabilities[b$true_class == 0])
deny <- density(b$probabilities[b$true_class == 1])


#den_unbound <- density(denx)
plot(denx, ylim = c(0, max(c(denx$y, deny$y))),
     xlim = c(min(c(denx$x, deny$x)),
              max(c(denx$x, deny$x))),
              main='Density plot of probabilities', xlab='probabilities')
lines(deny)

polygon(denx, col = adjustcolor("aquamarine", alpha.f=0.5) )
polygon(deny, col = adjustcolor("darkorange", alpha.f=0.5))
```


```{r}
with(prediction_table, table(observed, predicted))
```

```{r}
predicted <- predict(enet_linear_model, X_test, type='class')
prediction_table <- cbind(observed=y_test$nbc, predicted=as.numeric(predicted)) |> as.data.frame()
prediction_table[1:5, ]
```

```{r}
(with(prediction_table, observed - predicted)**2) |> mean()
```

```{r}
with(prediction_table, (max(predicted) - predicted)/(max(predicted) - min(predicted)))
```


### Testing on individuals

Read in the test set: LuCaP_145 and LuCaP_136
```{r}
test_data_dir <- glue('{project_dir}/data/train-test-val/freedman/data_2022-12-13')
```

```{r}
individual <- 'LuCaP_208' #'LuCaP_145'

test_data_path <- glue('{test_data_dir}/{individual}_aggByCenter_FOXA1.csv.gz')
test_data <- data.table::fread(test_data_path)
test_data[1:5, 1:5]
```

Ground truth (or class)
```{r}
gt_file <- Sys.glob(glue('{project_dir}/../impact_pipeline/motif_intervals/freedman/intervals_2022-12-11/ground_truth/{individual}_{TF}_*.txt'))

ground_truth <- data.table::fread(gt_file)
head(ground_truth)
```

```{r}
gt <- ground_truth[ground_truth$V1 %in% test_data$id, ]
dim(gt) ; length(unique(gt$V1))
```

```{r}
find_duplicates_in_dataframe <- function(dt, col, return_dups=TRUE){
  n_occur <- data.frame(table(dt[[col]]))

  if(return_dups == TRUE){
    return(dt[dt[[col]] %in% n_occur$Var1[n_occur$Freq > 1],])
  } else {
    return(n_occur[n_occur$Freq > 1,])
  }
}
```

```{r}
find_duplicates_in_dataframe(gt, col='V1')
```
Keep first occurrence of duplicates
```{r}
gt_dedup <- gt[!duplicated(gt[['V1']]),]
dim(gt_dedup)
```

Merge `gt_dedup` with the data
```{r}
new_dt <- merge(gt_dedup, test_data, by.x='V1', by.y='id')
new_dt[1:5, 1:5] ; dim(new_dt)
```

```{r}
colnames(new_dt)[1:3] <- c('region', 'class', 'binding_counts')
new_dt[1:5, 1:5] ; dim(new_dt)
```

```{r}
X_test <- new_dt[, -c(1:3)] |> as.matrix()
y_test <- new_dt[, c(1:3)] |> as.data.frame()

vbc <- y_test$binding_counts
y_test$nbc <- (vbc - min(vbc))/(max(vbc) - min(vbc))
```

```{r}
predicted <- predict(enet_binary_model, X_test, type='class')
prediction_table <- cbind(observed=y_test$class, predicted=as.numeric(predicted)) |> as.data.frame()
prediction_table[1:5, ]
```

```{r}
with(prediction_table, table(observed, predicted))
```

```{r}
predicted <- predict(enet_linear_model, X_test, type='class')
prediction_table <- cbind(observed=y_test$nbc, predicted=as.numeric(predicted)) |> as.data.frame()
prediction_table[1:5, ]
```

```{r}
(with(prediction_table, observed - predicted)**2) |> mean()
```

```{r}
predicted_values <- glmnet::predict.glmnet(enet_binary_balanced_model$glmnet.fit, X_test, type='class')
predicted_values[, 1] |> table()
```

```{r}
test_models(enet_binary_balanced_model, X_test, y_test$class)
```

```{r}
# list of individuals
individuals <- c('LuCaP_145', 'LuCaP_136', 'LuCaP_208')

y_test_list <- list()
X_test_list <- list()

for(ind in individuals){
  test_data_path <- glue('{test_data_dir}/{ind}_aggByCenter_FOXA1.csv.gz')
  test_data <- data.table::fread(test_data_path)
  gt_file <- Sys.glob(glue('{project_dir}/../impact_pipeline/motif_intervals/freedman/intervals_2022-12-11/ground_truth/{ind}_{TF}_*.txt'))
  ground_truth <- data.table::fread(gt_file)
  gt <- ground_truth[ground_truth$V1 %in% test_data$id, ]
  gt_dedup <- gt[!duplicated(gt[['V1']]),]
  new_dt <- merge(gt_dedup, test_data, by.x='V1', by.y='id')
  colnames(new_dt)[1:3] <- c('region', 'class', 'binding_counts')

  X_test <- new_dt[, -c(1:3)] |> as.matrix()
  y_test <- new_dt[, c(1:3)] |> as.data.frame()

  vbc <- y_test$binding_counts
  y_test$nbc <- (vbc - min(vbc))/(max(vbc) - min(vbc))

  y_test_list[[ind]] <- y_test
  X_test_list[[ind]] <- X_test
}

names(y_test_list) <- individuals
names(X_test_list) <- individuals
```

```{r}
source(glue('{project_dir}/scripts/utility-functions.R'))
```

```{r}
library(vcd)
test_metrics <- list()

# this produces a plot
lo <- layout(matrix(c(1, 2), nrow=1, ncol=2))

for(ind in individuals){

  test_metrics[[ind]] <- test_models(enet_binary_model, X_test_list[[ind]], y_test_list[[ind]]$class)

  test_predictions <- predict(enet_binary_model, X_test_list[[ind]], type='class')
  test_predictions <- as.numeric(test_predictions[, 1])

  mc <- cbind(test_predictions, y_test_list[[ind]]$class)
  colnames(mc) <- c('predicted', 'truth')
  mc <- as.data.frame(mc)

  mtable <- table(mc$predicted, mc$truth)
  dimnames(mtable) <- list(predicted = c('unbound', 'bound'), truth=c('unbound', 'bound'))

  # plot
  vcd::mosaic(mtable, highlighting = "truth", highlighting_fill = c("pink", "lightblue"), pop=F, main=ind)
  labeling_cells(text = mtable, margin=0)(mtable)
}
```

```{r}
test_metrics <- list()

# this produces a plot
lo <- layout(matrix(c(1, 2), nrow=1, ncol=2))

for(ind in individuals){

  test_metrics[[ind]] <- test_models(enet_binary_old_model, X_test_list[[ind]], y_test_list[[ind]]$class)

  test_predictions <- predict(enet_binary_old_model, X_test_list[[ind]], type='class')
  test_predictions <- as.numeric(test_predictions[, 1])

  mc <- cbind(test_predictions, y_test_list[[ind]]$class)
  colnames(mc) <- c('predicted', 'truth')
  mc <- as.data.frame(mc)

  mtable <- table(mc$predicted, mc$truth)
  dimnames(mtable) <- list(predicted = c('unbound', 'bound'), truth=c('unbound', 'bound'))

  # plot
  vcd::mosaic(mtable, highlighting = "truth", highlighting_fill = c("pink", "lightblue"), pop=F, main=ind)
  labeling_cells(text = mtable, margin=0)(mtable)
}
```

```{r}

for(ind in individuals){
  lo <- layout(matrix(c(1, 2), nrow=1, ncol=2))

  a <- predict(enet_binary_model, X_test_list[[ind]], type='response') #|> boxplot()
  b <- data.frame(probabilities=a[,1], class=y_test_list[[ind]]$class) #|> as.data.frame()

  boxplot(probabilities ~ class, data=b, col=c('aquamarine', 'darkorange'))
  mtext(glue('Boxplot of distribution of probabilities for {ind}'), line=1, cex=1.5)


  denx <- density(b$probabilities[b$class == 0])
  deny <- density(b$probabilities[b$class == 1])
  #den_unbound <- density(denx)
  plot(denx, ylim = c(0, max(c(denx$y, deny$y))),
      xlim = c(min(c(denx$x, deny$x)),
                max(c(denx$x, deny$x))), xlab='probabilities', main="")
  lines(deny)

  polygon(denx, col = adjustcolor("aquamarine", alpha.f=0.5))
  polygon(deny, col = adjustcolor("darkorange", alpha.f=0.5))
  mtext(glue('Density plot of probabilities for {ind}'), line=1, cex=1.5)
}
```


```{r}

```

```{r}
denx <- density(b$probabilities[b$true_class == 0])
deny <- density(b$probabilities[b$true_class == 1])


#den_unbound <- density(denx)
plot(denx, ylim = c(0, max(c(denx$y, deny$y))),
     xlim = c(min(c(denx$x, deny$x)),
              max(c(denx$x, deny$x))),
              main='Density plot of probabilities', xlab='probabilities')
lines(deny)

polygon(denx, col = adjustcolor("aquamarine", alpha.f=0.5) )
polygon(deny, col = adjustcolor("darkorange", alpha.f=0.5))
```



\newpage
## xgboost models
```{r}
library(xgboost)
library(DiagrammeR)
library(DiagrammeRsvg)
library(rsvg)
```
Create cv parameters
```{r}
# here we use a grid search to find the optimal hyperparameters
set.seed(2022)
xgb_grid_search <- expand.grid(colsample_bytree = c(0.2, 0.5, 0.7), max_depth=c(5, 20, 40), eta=c(0.01, 0.1, 1), gamma=c(20, 50), lambda=c(1, 10, 20), alpha=c(0, 10, 20))
xgb_grid_search <- xgb_grid_search[sample(1:nrow(xgb_grid_search), nrow(xgb_grid_search)), ]
xgb_grid_search[1:5, ] ; dim(xgb_grid_search)
```

```{r}
max_ <- 49
ids <- 1:nrow(xgb_grid_search)
ids_list <- split(xgb_grid_search, ceiling(ids/max_))

dir.create(glue('{project_dir}/metadata/cv_split'))
lapply(seq_along(ids_list), function(i){
  write.table(xgb_grid_search, file=glue('{project_dir}/metadata/cv_split/xgb_gridcv_parameters_{i}.txt'), quote=F, row.names=F, col.names=F)
})
```

```{r}
list.files(glue('{project_dir}/metadata/cv_split'))
```



```{r}
write.table(xgb_grid_search[1:10, ], file=glue('{project_dir}/metadata/xgb_gridcv_parameters.txt'), quote=F, row.names=F, col.names=F)
```

I have moved this section to a stand-alone script that trains and saves the model. It takes some time to build the model and I needed some compute power.
```{r}
# path to the data
training_data <- paste0(project_dir, '/data/enet_data/data_2022-12-16/train_enet_2022-12-16.csv.gz')
training_data ; file.exists(training_data)
```

```{r}
# path to cv data
cv_params <- glue('{project_dir}/metadata/xgb_gridcv_parameters.txt')
cv_params ; file.exists(cv_params)
```

```{r}
mtinfo <- 'single_params'
param_num <- 0
```

```{r}
xgboost_pbs_script <- glue('qsub -v data_file={training_data},grid_parameters={cv_params},metainfo={mtinfo},param_num={i} {project_dir}/scripts/xgboost/cv_xgboost_pbs.sh')
xgboost_pbs_script #; file.exists(pbs_script)
```

```{r}
system(xgboost_pbs_script)
```
```{r}
system("qstat -u temi")
```

#### Can run larger cross validations this way
```{r}
# here we use a grid search to find the optimal hyperparameters
set.seed(2022)
xgb_grid_search <- expand.grid(colsample_bytree = c(0.2, 0.5, 0.7), max_depth=c(5, 20, 40), eta=c(0.01, 0.1, 1), gamma=c(20, 50), lambda=c(1, 10, 20), alpha=c(0, 10, 20))
xgb_grid_search <- xgb_grid_search[sample(1:nrow(xgb_grid_search), nrow(xgb_grid_search)), ]
xgb_grid_search[1:5, ] ; dim(xgb_grid_search)
```

```{r}
# max_ <- 10 # 49 is the max for the mediumn queue ; change to 24 for small queue
# ids <- 1:nrow(xgb_grid_search)
# ids_list <- split(xgb_grid_search, ceiling(ids/max_))

ids <- sample(1:max_, nrow(xgb_grid_search), replace=T) |> sort()
ids_list <- split(xgb_grid_search, ids)

dir.create(glue('{project_dir}/metadata/cv_split'))
lapply(seq_along(ids_list), function(i){
  write.table(ids_list[[i]], file=glue('{project_dir}/metadata/cv_split/xgb_gridcv_parameters_{i}.txt'), quote=F, row.names=F, col.names=F)
})
```

```{r}
list.files(glue('{project_dir}/metadata/cv_split'))
```

```{r}
# write out the full paths of the params you want to batch run
list.files(glue('{project_dir}/metadata/cv_split'), full.names=T) |> cat(file=glue('{project_dir}/metadata/params_files.txt'), sep='\n')
```

```{r}
cv_params <- glue('{project_dir}/metadata/params_files.txt')
mtinfo <- 'batch_run'
```

```{r}
xgboost_pbs_script <- glue('qsub -v data_file={training_data},grid_parameters={cv_params},metainfo={mtinfo} {project_dir}/scripts/xgboost/cv_xgboost_pbs.sh')
system(xgboost_pbs_script)
```


```{r}
#mtinfo <- 'batch_params'
for(i in 1:10){
  #cv_params <- glue('{project_dir}/metadata/cv_split/xgb_gridcv_parameters_{i}.txt')
  xgboost_pbs_script <- glue('qsub -v data_file={training_data},grid_parameters={cv_params},metainfo={mtinfo},param_num={i} {project_dir}/scripts/xgboost/cv_xgboost_pbs.sh')
  print(xgboost_pbs_script)
}
```

```{r}
system("qstat -u temi")
```


#####

```{r}
set.seed(2022)
xgb_grid_search <- expand.grid(colsample_bytree = c(0.2), max_depth=c(5), eta=c(0.01), gamma=c(20, 50), lambda=c(1, 20), alpha=c(10, 20))
xgb_grid_search <- xgb_grid_search[sample(1:nrow(xgb_grid_search), nrow(xgb_grid_search)), ] |> unname()
xgb_grid_search[1:5, ] ; dim(xgb_grid_search)
```

```{r}
grid_model <- apply(xgb_grid_search[1:2, ], 1, function(xgb_arguments){

  xgb_params <- list(colsample_bytree=xgb_arguments[1], max_depth=xgb_arguments[2], eta=xgb_arguments[3], gamma=xgb_arguments[4], lambda=xgb_arguments[5], alpha=xgb_arguments[6])
  params <- c(xgb_params, list(objective = "binary:logistic", eval_metric='auc'))

  #print(xgb_params)
  # uses 3-folds cross validation
  xgb_model <- xgboost::xgb.cv(data=dtrain, params=params, nrounds=5, nfold=3, metrics='auc', early_stopping_rounds = 20, verbose=0)

  scores <- as.data.frame(xgb_model$evaluation_log)
  test_auc <- tail(scores$test_auc_mean, 1)
  train_auc <- tail(scores$train_auc_mean, 1)

  return(c(test_auc=test_auc, train_auc=train_auc, unlist(xgb_params)))



}) |> t()

grid_model
```

####

```{r}
set.seed(2022)
n <- 500
x1 <- rnorm(n, 3, 0.5)
x2 <- runif(n, 0, 20)
X_train <- cbind(x1, x2)
y_train <- ifelse((x1 + x2)**(x1 - x2) < mean(x1), 0, 1)
#dtrain <- xgboost::xgb.DMatrix(data=X_train, label=y_train)
```
```{r}
parallel::detectCores()
ncores <- 3
```

```{r}
# register workers/cores
print(glue('[INFO] Detected {parallel::detectCores()} cores.'))
cl <- parallel::makeForkCluster(ncores, outfile = glue('{project_dir}/log/foreach.out'))
doParallel::registerDoParallel(cl)
print(glue('[INFO] Registered {ncores} cores.'))
#print(glue('Cluster call: {clusterCall(cl, function() Sys.info()[c("nodename","machine")])}'))
```

```{r}
params_data <- data.table::fread(glue('{project_dir}/metadata/cv_split/xgb_gridcv_parameters_1.txt'), header=F)[1:3, ] #|> unname()
params_data
```

```{r}
# set up param list # .export=ls(globalenv())
#print(glue('[INFO] Starting cross-validation for batch {batch_num}'))
xgb_grid_result <- foreach::foreach(i=1:nrow(params_data), .combine='rbind') %dopar% {
  print(glue('[INFO] Currently on row {i}'))

  # dtrain <- xgboost::xgb.DMatrix(data=X, label=y)
  dtrain <- xgboost::xgb.DMatrix(data=X_train, label=y_train)

  xgb_params <- list(colsample_bytree=params_data[i,1], max_depth=params_data[i,2], eta=params_data[i,3], gamma=params_data[i,4], lambda=params_data[i,5], alpha=params_data[i,6])
  params <- c(xgb_params, list(objective = "binary:logistic", eval_metric='auc'))

  print(params |> unlist())

  # xgb_model <- xgboost::xgb.cv(data=dtrain, params=params, nrounds=20, nfold=3, metrics='auc', early_stopping_rounds = 10)

  # scores <- as.data.frame(xgb_model$evaluation_log)
  # idmax_test_auc <- which.max(scores$test_auc_mean)
  # test_auc <- scores$test_auc_mean[idmax_test_auc]
  # train_auc <- scores$train_auc_mean[idmax_test_auc]

  # return(c(test_auc=test_auc, train_auc=train_auc, unlist(xgb_params)))
}

parallel::stopCluster(cl)
registerDoSEQ()
```

```{r}
xgb_grid_result <- parallel::mclapply(seq(1:nrow(params_data)), function(i){
    print(glue('[INFO] Currently on row {i}'))

    # dtrain <- xgboost::xgb.DMatrix(data=X, label=y)
    dtrain <- xgboost::xgb.DMatrix(data=X_train, label=y_train)

    xgb_params <- list(colsample_bytree=params_data[i,1], max_depth=params_data[i,2], eta=params_data[i,3], gamma=params_data[i,4], lambda=params_data[i,5], alpha=params_data[i,6])
    params <- c(xgb_params, list(objective = "binary:logistic", eval_metric='auc'))

    xgb_model <- xgboost::xgb.cv(data=dtrain, params=params, nrounds=20, nfold=3, metrics='auc', early_stopping_rounds = 10)

    scores <- as.data.frame(xgb_model$evaluation_log)
    idmax_test_auc <- which.max(scores$test_auc_mean)
    test_auc <- scores$test_auc_mean[idmax_test_auc]
    train_auc <- scores$train_auc_mean[idmax_test_auc]

    return(c(test_auc=test_auc, train_auc=train_auc, unlist(xgb_params)))
}, mc.cores=ncores) 

xgb_grid_result <- do.call('rbind', xgb_grid_result)

xgb_grid_result
```


### Split the data

```{r}
data_file <- "/lus/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan/modeling_pipeline/data/train-test-val/kawakami/data_2022-12-16/kawakami_aggByCenter_FOXA1.csv.gz"
train_data <- data.table::fread(data_file)
# X <- train_data[, -c(1,2,3)] |> as.matrix()
# y <- train_data$class
```

```{r}
small_train_data <- train_data[1:500, ]
small_train_data |> dim()
```

```{r}
small_train_data$class |> table()
```




```{r}
n <- 200
x1 <- rnorm(n, 6, 1)
x2 <- runif(n, 1, 10)
X_test <- cbind(x1, x2)
y_test <- ifelse((x1 + x2)**(x1 - x2) < mean(x1), 0, 1)
dtest <- xgboost::xgb.DMatrix(data=X_test, label=y_test)
```

```{r}
params_1 <- xgb_grid_search[1, ] |> unlist() |> as.list()
params_list <- c(params_1, list(verbosity=0, objective = "binary:logistic", eval_metric='auc', booster='gbtree'))
```

```{r}
xgb_cv_run <- xgboost::xgb.cv(data=dtrain, params=params_list, nrounds=25, nfold=2, metrics='auc', early_stopping_rounds = 10, nthread=1, verbose=0)
xgb_cv_run
```

```{r}
threads <- c(1:25)
#lapply(threads, function(nt){
for(nt in threads){
  start_time <- Sys.time()
  xgb_cv_run <- xgboost::xgb.cv(data=dtrain, params=params_list, nrounds=25, nfold=2, metrics='auc', early_stopping_rounds = 10, nthread=nt, verbose=0)
  end_time <- Sys.time()

  print(round(end_time - start_time, 2))
}

```


```{r}
xgb_model_trained <- xgb.train(data=dtrain, params=params_list, nrounds=50, watchlist=list(training=dtrain, testing=dtest), print_every_n = 10, early_stopping_rounds = 20, verbose=0)
xgb_model_trained
```







```{r}
# I should try with a small dataset

# using the aggByCenter 
kawakami_center_dt <- data.table::fread(glue('{kawakami_data_dir}/aggByCenter_{TF}_40000.csv'))

# split the data
X_train <- kawakami_center_dt[, -c(1,2)] |> as.matrix()
y_train <- kawakami_center_dt[, c(1,2)] |> as.data.frame()

# check the distribution
y_train$class |> table()
```

```{r}
# for optimization, I need a test set
set.seed(27102022)

# first bind the data 
dt_all <- cbind(y_train[, 2], X_train) |> as.data.frame()

# split and do your magic
split_dt <- lapply(split(dt_all, f=dt_all$V1), function(each_dt){

  dt_indices <- sample(1:nrow(each_dt), 5000)
  a <- each_dt[dt_indices, ] 
  b <- each_dt[-dt_indices, ]

  return(list(train=a, test=b))

})

train_dt <- rbind(split_dt[[1]][[1]], split_dt[[2]][[1]])
test_dt <- rbind(split_dt[[1]][[2]], split_dt[[2]][[2]])

train_dt <- train_dt[sample(1:nrow(train_dt)), ] |> as.matrix()
test_dt <- test_dt[sample(1:nrow(test_dt)), ] |> as.matrix()

dim(train_dt) ; dim(test_dt)
```

```{r}
# construct a dmatrix
dtrain <- xgboost::xgb.DMatrix(data=train_dt[, -1], label=train_dt[, 1])
dtest <- xgboost::xgb.DMatrix(data=test_dt[, -1], label=test_dt[, 1])
```

```{r, eval=F}
# here we use a grid search to find the optimal hyperparameters
xgb_grid_search <- expand.grid(colsample_bytree = 0.7, max_depth=20, eta=c(0.01, 0.1, 1), gamma=c(5, 10, 20), lambda=c(1, 3, 5), alpha=c(0, 3, 7))
xgb_grid_search
```

```{r, eval=F}
grid_model <- apply(xgb_grid_search, 1, function(each_param){

  params <- list(max_depth=each_param[['max_depth']], eta=each_param[['eta']], colsample_bytree=each_param[['colsample_bytree']],
    gamma=each_param[['gamma']], lambda=each_param[['lambda']], nthreads=8, verbosity=0, objective = "binary:logistic", eval_metric='auc')

  # uses 3-folds cross validation
  xgb_model <- xgboost::xgb.cv(data=dtrain, params=params, nrounds=100, nfold=5, metrics='auc',
    early_stopping_rounds = 20, verbose = 1, print_every_n = 20)

  scores <- as.data.frame(xgb_model$evaluation_log)
  test_auc <- tail(scores$test_auc_mean, 1)
  train_auc <- tail(scores$train_auc_mean, 1)

  return(c(test_auc, train_auc, each_param[['max_depth']], each_param[['eta']], each_param[['colsample_bytree']], each_param[['lambda']], each_param[['gamma']]))

})
```

```{r, eval=F}
grid_result <- t(grid_model) |> as.data.frame()
colnames(grid_result) <- c('test_auc', 'train_auc', 'max_depth', 'learning_rate', 'colsample_bytree', 'lambda', 'gamma')
```


```{r}
#test_boost <- xgboost(dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 50, objective = "binary:logistic", eval_metric='auc')

# params <- list(colsample_bytree = 0.7, max_depth=50, eta=c(0.01), gamma=20, lambda=25)
# xgb_model <- xgboost::xgb.cv(data=dtrain, params=params, nrounds=300, nfold=3, metrics='auc',
#     early_stopping_rounds = 10, verbose = 2, print_every_n = 1)
```

```{r}
params <- list(colsample_bytree = 0.7, subsample=0.7, max_depth=30, eta=c(0.1), gamma=25, lambda=3, alpha=3, nthread=8, verbosity=1, objective = "binary:logistic", eval_metric=c('auc', 'aucpr', 'error'))
xgb_model_trained <- xgb.train(data=dtrain, params=params, nrounds=600, watchlist=list(training=dtrain, testing=dtest), print_every_n = 10, early_stopping_rounds = 50)
xgb_model_trained
```

```{r}
plot(xgb_model_trained$evaluation_log$training_auc, type='l', ylim=c(0.7, 1))
lines(xgb_model_trained$evaluation_log$testing_auc, col='red')
```


```{r}
imp_matrix <- xgb.importance(model = xgb_model_trained)
```

```{r}
xgb.plot.importance(importance_matrix = imp_matrix)
```

```{r}
gr <- xgb.plot.tree(model = xgb_model_trained, tree=1, render=F)
export_graph(gr, file_name=glue('{project_dir}/plots/tree_1_node.png'), file_type='png')
```

```{r}
library(SHAPforxgboost)
```

obtain shap values

```{r}
shap_values <- SHAPforxgboost::shap.values(xgb_model_trained, train_dt[, -1])#predict(xgb_model_trained, train_dt[, -1], predcontrib = TRUE, approxcontrib = F)
```



```{r}
save_dir = '/projects/covid-ct/imlab/users/temi/projects/TFXcan/train-test-val/freedman'
ind <- 'LuCaP_145'
test_data_path <- glue('{save_dir}/{ind}_aggByCenter_FOXA1.csv')
val_data <- data.table::fread(test_data_path)

X_val <- val_data[, -c(1:2)] |> as.matrix()
y_val <- val_data[, c(1:2)] |> as.data.frame()
```

```{r}
xgb_prediction <- predict(xgb_model_trained, X_val)
xgb_pred_class <- ifelse(xgb_prediction > 0.5, 1, 0)
```

```{r}
table(xgb_pred_class, y_val[, 2])
```

\newpage

I mostly used this to create a script that I can run on the cluster

```{r}
# here we use a grid search to find the optimal hyperparameters
xgb_grid_search <- expand.grid(colsample_bytree = 0.7, max_depth=20, eta=c(0.01, 0.1, 1), gamma=c(5, 10, 20), lambda=c(1, 3, 5), alpha=c(0, 3, 7))
xgb_grid_search
```

```{r}
# testing for each
#fk_cl <- parallel::makeForkCluster(8)
fk_cl <- parallel::makeCluster(8)
registerDoParallel(fk_cl)

#clusterExport(fk_cl, c('dtrain'))
#clusterEvalQ(fk_cl, 'dtrain')

set.seed(27102022)
#user_matrix <- matrix(sample(1:500, 10*10), nrow=10, ncol=10)
output_cv_result <- foreach(i=1:nrow(xgb_grid_search), .combine='rbind', .packages=c('xgboost'), .inorder=F) %dopar% {

  # this is important - you need to initialize this within the loop !!!
  dtrain <- xgboost::xgb.DMatrix(data=train_dt[, -1], label=train_dt[, 1])

  params <- list(max_depth=xgb_grid_search[i, 'max_depth'], eta=xgb_grid_search[i, 'eta'], colsample_bytree=xgb_grid_search[i, 'colsample_bytree'], gamma=xgb_grid_search[i, 'gamma'], lambda=xgb_grid_search[i, 'lambda'], alpha=xgb_grid_search[i, 'alpha'], nthreads=2, verbosity=0, objective = "binary:logistic", eval_metric='auc', booster='gbtree')

  # # uses 3-folds cross validation
  xgb_model <- xgboost::xgb.cv(data=dtrain, params=params, nrounds=5, nfold=3, metrics='auc', early_stopping_rounds = 20, print_every_n = 1)

  scores <- as.data.frame(xgb_model$evaluation_log)
  test_auc <- tail(scores$test_auc_mean, 1)
  train_auc <- tail(scores$train_auc_mean, 1)

  return(
    c(test_auc, train_auc, 
    xgb_grid_search[i, 'max_depth'], xgb_grid_search[i, 'eta'], 
    xgb_grid_search[i, 'colsample_bytree'], xgb_grid_search[i, 'gamma'], 
    xgb_grid_search[i, 'lambda'], xgb_grid_search[i, 'alpha'])
  )
}

stopCluster(fk_cl)
```