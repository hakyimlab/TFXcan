---
title: "Build models on Kawakami data: elastic net and xgboost"
author: "Temi"
date: 'Wednesday Oct 26 2022'
format: 
  pdf: 
    toc: true
    number-sections: true
    code-line-numbers: true
---

```{r}
rm(list=ls())

library(glue)
library(reticulate)
library(R.utils)
library(data.table)
library(glmnet)
library(doMC)
library(ROCR)
library(Matrix)
library(reshape2)
library(tidyverse)
library(foreach)
library(doParallel)

```

```{r}
project_dir <- '/lus/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan/modeling_pipeline'
setwd(project_dir)

TF <- 'FOXA1'
id <- 'kawakami'
kawakami_data_dir <- glue('{project_dir}/data/train-test-val/kawakami/data_2022-12-12')
```

```{r}
# using the aggByCenter 
kawakami_center_dt <- data.table::fread(glue('{kawakami_data_dir}/{id}_aggByCenter_{TF}.csv.gz'))
dim(kawakami_center_dt) ; kawakami_center_dt[1:5, 1:5]
```

Ground truth (or class)

```{r}
ground_truth_path <- Sys.glob(paste0(project_dir, '/../impact_pipeline/motif_intervals/kawakami/intervals_2022-12-11/ground_truth/', id, '_', TF, '_*.txt'))
ground_truth_path
```

```{r}
ground_truth <- data.table::fread(ground_truth_path)
head(ground_truth)
```

```{r}
gt <- ground_truth[ground_truth$V1 %in% kawakami_center_dt$id, ]
dim(gt) ; length(unique(gt$V1))
```

```{r}
find_duplicates_in_dataframe <- function(dt, col, return_dups=TRUE){
  n_occur <- data.frame(table(dt[[col]]))

  if(return_dups == TRUE){
    return(dt[dt[[col]] %in% n_occur$Var1[n_occur$Freq > 1],])
  } else {
    return(n_occur[n_occur$Freq > 1,])
  }
}
```

```{r}
find_duplicates_in_dataframe(gt, col='V1')
```
Keep first occurrence of duplicates
```{r}
gt_dedup <- gt[!duplicated(gt[['V1']]),]
dim(gt_dedup)
```

Merge `gt_dedup` with the data
```{r}
new_dt <- merge(gt_dedup, kawakami_center_dt, by.x='V1', by.y='id')
new_dt[1:5, 1:5]
```

```{r}
#new_dt <- new_dt[, -c('class')]
colnames(new_dt)[1:3] <- c('region', 'class', 'binding_counts')
new_dt[1:5, 1:5] ; dim(new_dt)
```

Split into 80-20
```{r}
set.seed(2022)
tr_size <- ceiling(nrow(new_dt) * 0.8)
tr_indices <- sample(1:nrow(new_dt), tr_size)
```

```{r}
train <- new_dt[tr_indices, ]
test <- new_dt[-tr_indices, ]
```
Save the data
```{r}
save_dir <- paste0(project_dir, '/data/enet_data/data_', Sys.Date())
if(!dir.exists(save_dir)){
  dir.create(save_dir)
}
```
```{r}
write.csv(x=train, file=gzfile(paste0(save_dir, '/train_enet.csv.gz')))
write.csv(x=test, file=gzfile(paste0(save_dir, '/test_enet.csv.gz')))
```



```{r}
# split the data

X_train <- kawakami_center_dt[, -c(1,2)] |> as.matrix()
y_train <- kawakami_center_dt[, c(1,2)] |> as.data.frame()
```

```{r}
# check the distribution
y_train$class |> table()
```

### Utility functions
```{r}
source(glue('{project_dir}/scripts/utility-functions.R'))
```

```{r}
dt_train <- data.table::fread(paste0(project_dir, '/data/enet_data/data_2022-12-13/balanced_enet.csv.gz'))
X_train <- dt_train[, -c(1,2,3)] |> as.matrix()
y_train <- dt_train[, c(1,2,3)] |> as.data.frame()
```

```{r}
log10(freedman_linear_truth$num + 1)/sum(log10(freedman_linear_truth$num + 1))
```

```{r}
vbc <- log10(y_test$binding_counts + 1)
nbc <- vbc/sum(vbc)
```

```{r}
vbc <- y_test$binding_counts
nbc <- (vbc - min(vbc))/(max(vbc) - min(vbc))
```

```{r}
nbc[nbc > 0.9]
```





\newpage
### Modelling: elastic net
I have moved this section to a stand-alone script that trains and saves the model. It takes some time to build the model and I needed some compute power.
```{r}
# path to the data
training_data <- paste0(project_dir, '/data/enet_data/data_2022-12-13/balanced_enet.csv.gz')
training_data
```

```{r}
pbs_script <- glue('qsub -v "data_file={training_data}" {project_dir}/scripts/enet/train_enet_model_pbs.sh')
pbs_script
```
<<<<<<< HEAD

```{r}
system(pbs_script)
=======

```{r}
system(pbs_script)
```

```{r}
system("qstat -u temi")
>>>>>>> 36da6727ffb7caab6253ac17f3c21d86f6a76839
```

Read in the model(s)
```{r}
<<<<<<< HEAD
system("qstat -u temi")
```

Read in the model(s)
```{r}
=======
>>>>>>> 36da6727ffb7caab6253ac17f3c21d86f6a76839
enet_binary_model <- readRDS(paste0(project_dir, '/models/enet_models/kawakami_center_binary_balanced_2022-12-14.rds'))
enet_binary_model
```

```{r}
enet_linear_model <- readRDS(paste0(project_dir, '/models/enet_models/kawakami_center_linear_balanced_2022-12-14.rds'))
enet_linear_model
```

```{r}
enet_binary_old_model <- readRDS(paste0(project_dir, '/models/enet_models/kawakami_center_binary_old_2022-12-14.rds'))
enet_binary_old_model
```

<<<<<<< HEAD
=======



>>>>>>> 36da6727ffb7caab6253ac17f3c21d86f6a76839
Test on the training data itself
```{r}
#test_data <- data.table::fread(paste0(project_dir, '/data/enet_data/data_', Sys.Date(), '/balanced_enet.csv.gz'))
test_data <- data.table::fread(training_data)
X_test <- test_data[, -c(1,2,3)] |> as.matrix()
y_test <- test_data[, c(1,2,3)] |> as.data.frame()

vbc <- y_test$binding_counts
y_test$nbc <- (vbc - min(vbc))/(max(vbc) - min(vbc))

X_test |> dim() ; X_test[1:5, 1:5] ; y_test |> head()
<<<<<<< HEAD
=======
```

```{r}
tr_data <- data.table::fread('/lus/grand/projects/covid-ct/imlab/users/temi/projects/TFXcan/modeling_pipeline/data/train-test-val/kawakami/data_2022-12-12/kawakami_aggByCenter_FOXA1_old.csv.gz')
```

```{r}
tr_data[1:5, 1:5]
```

### Some fancy plots
```{r}
a <- predict(enet_binary_old_model, tr_data[, -1] |> as.matrix(), type='response') #|> boxplot()
b <- data.frame(probabilities=a[,1], true_class=tr_data[, 1])
```

```{r}
boxplot(probabilities ~ class, data=b, col=c('aquamarine', 'darkorange'))
mtext('Boxplot of distribution of probabilities', line=1, cex=1.5)
```

```{r}
denx <- density(b$probabilities[b$class == 0])
deny <- density(b$probabilities[b$class == 1])


#den_unbound <- density(denx)
plot(denx, ylim = c(0, max(c(denx$y, deny$y))),
     xlim = c(min(c(denx$x, deny$x)),
              max(c(denx$x, deny$x))),
              main='Density plot of probabilities', xlab='probabilities')
lines(deny)

polygon(denx, col = adjustcolor("aquamarine", alpha.f=0.5) )
polygon(deny, col = adjustcolor("darkorange", alpha.f=0.5))
>>>>>>> 36da6727ffb7caab6253ac17f3c21d86f6a76839
```

```{r}
predicted <- predict(enet_binary_model, X_test, type='class')
prediction_table <- cbind(observed=y_test$class, predicted=as.numeric(predicted)) |> as.data.frame()
<<<<<<< HEAD
prediction_table[1:5, ]
=======
prediction_table[1:5, ] ; with(prediction_table, table(observed, predicted))
```

```{r}
predicted <- predict(enet_binary_old_model, X_test, type='class')
prediction_table <- cbind(observed=y_test$class, predicted=as.numeric(predicted)) |> as.data.frame()
prediction_table[1:5, ] ; with(prediction_table, table(observed, predicted))
>>>>>>> 36da6727ffb7caab6253ac17f3c21d86f6a76839
```

### Some fancy plots
```{r}
<<<<<<< HEAD
with(prediction_table, table(observed, predicted))
```

```{r}
predicted <- predict(enet_linear_model, X_test, type='class')
prediction_table <- cbind(observed=y_test$nbc, predicted=as.numeric(predicted)) |> as.data.frame()
prediction_table[1:5, ]
```

```{r}
(with(prediction_table, observed - predicted)**2) |> mean()
```

```{r}
with(prediction_table, (max(predicted) - predicted)/(max(predicted) - min(predicted)))
```


Read in the test set: LuCaP_145 and LuCaP_136
```{r}
test_data_dir <- glue('{project_dir}/data/train-test-val/freedman/data_2022-12-13')
```

```{r}
individual <- 'LuCaP_208' #'LuCaP_145'

test_data_path <- glue('{test_data_dir}/{individual}_aggByCenter_FOXA1.csv.gz')
test_data <- data.table::fread(test_data_path)
test_data[1:5, 1:5]
```

Ground truth (or class)
```{r}
gt_file <- Sys.glob(glue('{project_dir}/../impact_pipeline/motif_intervals/freedman/intervals_2022-12-11/ground_truth/{individual}_{TF}_*.txt'))

ground_truth <- data.table::fread(gt_file)
head(ground_truth)
```

```{r}
gt <- ground_truth[ground_truth$V1 %in% test_data$id, ]
dim(gt) ; length(unique(gt$V1))
```

```{r}
find_duplicates_in_dataframe <- function(dt, col, return_dups=TRUE){
  n_occur <- data.frame(table(dt[[col]]))

  if(return_dups == TRUE){
    return(dt[dt[[col]] %in% n_occur$Var1[n_occur$Freq > 1],])
  } else {
    return(n_occur[n_occur$Freq > 1,])
  }
}
```

```{r}
find_duplicates_in_dataframe(gt, col='V1')
```
Keep first occurrence of duplicates
```{r}
gt_dedup <- gt[!duplicated(gt[['V1']]),]
dim(gt_dedup)
```

Merge `gt_dedup` with the data
```{r}
new_dt <- merge(gt_dedup, test_data, by.x='V1', by.y='id')
new_dt[1:5, 1:5] ; dim(new_dt)
```

```{r}
colnames(new_dt)[1:3] <- c('region', 'class', 'binding_counts')
new_dt[1:5, 1:5] ; dim(new_dt)
```

```{r}
X_test <- new_dt[, -c(1:3)] |> as.matrix()
y_test <- new_dt[, c(1:3)] |> as.data.frame()
```

```{r}
predicted_values <- glmnet::predict.glmnet(enet_binary_balanced_model$glmnet.fit, X_test, type='class')
predicted_values[, 1] |> table()
```


```{r}
test_models(enet_binary_balanced_model, X_test, y_test$class)
=======
a <- predict(enet_binary_model, X_test, type='response') #|> boxplot()
b <- cbind(probabilities=a[,1], true_class=y_test$class) |> as.data.frame()
```

```{r}
boxplot(probabilities ~ true_class, data=b, col=c('aquamarine', 'darkorange'))
mtext('Boxplot of distribution of probabilities', line=1, cex=1.5)
```

```{r}
denx <- density(b$probabilities[b$true_class == 0])
deny <- density(b$probabilities[b$true_class == 1])


#den_unbound <- density(denx)
plot(denx, ylim = c(0, max(c(denx$y, deny$y))),
     xlim = c(min(c(denx$x, deny$x)),
              max(c(denx$x, deny$x))),
              main='Density plot of probabilities', xlab='probabilities')
lines(deny)

polygon(denx, col = adjustcolor("aquamarine", alpha.f=0.5) )
polygon(deny, col = adjustcolor("darkorange", alpha.f=0.5))
>>>>>>> 36da6727ffb7caab6253ac17f3c21d86f6a76839
```


```{r}
with(prediction_table, table(observed, predicted))
```

```{r}
predicted <- predict(enet_linear_model, X_test, type='class')
prediction_table <- cbind(observed=y_test$nbc, predicted=as.numeric(predicted)) |> as.data.frame()
prediction_table[1:5, ]
```

```{r}
(with(prediction_table, observed - predicted)**2) |> mean()
```

```{r}
with(prediction_table, (max(predicted) - predicted)/(max(predicted) - min(predicted)))
```


### Testing on individuals

Read in the test set: LuCaP_145 and LuCaP_136
```{r}
test_data_dir <- glue('{project_dir}/data/train-test-val/freedman/data_2022-12-13')
```

```{r}
individual <- 'LuCaP_208' #'LuCaP_145'

test_data_path <- glue('{test_data_dir}/{individual}_aggByCenter_FOXA1.csv.gz')
test_data <- data.table::fread(test_data_path)
test_data[1:5, 1:5]
```

Ground truth (or class)
```{r}
gt_file <- Sys.glob(glue('{project_dir}/../impact_pipeline/motif_intervals/freedman/intervals_2022-12-11/ground_truth/{individual}_{TF}_*.txt'))

ground_truth <- data.table::fread(gt_file)
head(ground_truth)
```

```{r}
gt <- ground_truth[ground_truth$V1 %in% test_data$id, ]
dim(gt) ; length(unique(gt$V1))
```

```{r}
find_duplicates_in_dataframe <- function(dt, col, return_dups=TRUE){
  n_occur <- data.frame(table(dt[[col]]))

  if(return_dups == TRUE){
    return(dt[dt[[col]] %in% n_occur$Var1[n_occur$Freq > 1],])
  } else {
    return(n_occur[n_occur$Freq > 1,])
  }
}
```

```{r}
find_duplicates_in_dataframe(gt, col='V1')
```
Keep first occurrence of duplicates
```{r}
gt_dedup <- gt[!duplicated(gt[['V1']]),]
dim(gt_dedup)
```

Merge `gt_dedup` with the data
```{r}
new_dt <- merge(gt_dedup, test_data, by.x='V1', by.y='id')
new_dt[1:5, 1:5] ; dim(new_dt)
```

```{r}
colnames(new_dt)[1:3] <- c('region', 'class', 'binding_counts')
new_dt[1:5, 1:5] ; dim(new_dt)
```

```{r}
X_test <- new_dt[, -c(1:3)] |> as.matrix()
y_test <- new_dt[, c(1:3)] |> as.data.frame()

vbc <- y_test$binding_counts
y_test$nbc <- (vbc - min(vbc))/(max(vbc) - min(vbc))
```

```{r}
predicted <- predict(enet_binary_model, X_test, type='class')
prediction_table <- cbind(observed=y_test$class, predicted=as.numeric(predicted)) |> as.data.frame()
prediction_table[1:5, ]
```

```{r}
with(prediction_table, table(observed, predicted))
```

```{r}
predicted <- predict(enet_linear_model, X_test, type='class')
prediction_table <- cbind(observed=y_test$nbc, predicted=as.numeric(predicted)) |> as.data.frame()
prediction_table[1:5, ]
```

```{r}
(with(prediction_table, observed - predicted)**2) |> mean()
```

```{r}
predicted_values <- glmnet::predict.glmnet(enet_binary_balanced_model$glmnet.fit, X_test, type='class')
predicted_values[, 1] |> table()
```

```{r}
test_models(enet_binary_balanced_model, X_test, y_test$class)
```

```{r}
# list of individuals
individuals <- c('LuCaP_145', 'LuCaP_136', 'LuCaP_208')

y_test_list <- list()
X_test_list <- list()

for(ind in individuals){
  test_data_path <- glue('{test_data_dir}/{ind}_aggByCenter_FOXA1.csv.gz')
  test_data <- data.table::fread(test_data_path)
  gt_file <- Sys.glob(glue('{project_dir}/../impact_pipeline/motif_intervals/freedman/intervals_2022-12-11/ground_truth/{ind}_{TF}_*.txt'))
  ground_truth <- data.table::fread(gt_file)
  gt <- ground_truth[ground_truth$V1 %in% test_data$id, ]
  gt_dedup <- gt[!duplicated(gt[['V1']]),]
  new_dt <- merge(gt_dedup, test_data, by.x='V1', by.y='id')
  colnames(new_dt)[1:3] <- c('region', 'class', 'binding_counts')

  X_test <- new_dt[, -c(1:3)] |> as.matrix()
  y_test <- new_dt[, c(1:3)] |> as.data.frame()

  vbc <- y_test$binding_counts
  y_test$nbc <- (vbc - min(vbc))/(max(vbc) - min(vbc))

  y_test_list[[ind]] <- y_test
  X_test_list[[ind]] <- X_test
}

names(y_test_list) <- individuals
names(X_test_list) <- individuals
```

```{r}
source(glue('{project_dir}/scripts/utility-functions.R'))
```

```{r}
library(vcd)
test_metrics <- list()

# this produces a plot
lo <- layout(matrix(c(1, 2), nrow=1, ncol=2))

for(ind in individuals){

  test_metrics[[ind]] <- test_models(enet_binary_model, X_test_list[[ind]], y_test_list[[ind]]$class)

  test_predictions <- predict(enet_binary_model, X_test_list[[ind]], type='class')
  test_predictions <- as.numeric(test_predictions[, 1])

  mc <- cbind(test_predictions, y_test_list[[ind]]$class)
  colnames(mc) <- c('predicted', 'truth')
  mc <- as.data.frame(mc)

  mtable <- table(mc$predicted, mc$truth)
  dimnames(mtable) <- list(predicted = c('unbound', 'bound'), truth=c('unbound', 'bound'))

  # plot
  vcd::mosaic(mtable, highlighting = "truth", highlighting_fill = c("pink", "lightblue"), pop=F, main=ind)
  labeling_cells(text = mtable, margin=0)(mtable)
}
```

```{r}
test_metrics <- list()

# this produces a plot
lo <- layout(matrix(c(1, 2), nrow=1, ncol=2))

for(ind in individuals){

  test_metrics[[ind]] <- test_models(enet_binary_old_model, X_test_list[[ind]], y_test_list[[ind]]$class)

  test_predictions <- predict(enet_binary_old_model, X_test_list[[ind]], type='class')
  test_predictions <- as.numeric(test_predictions[, 1])

  mc <- cbind(test_predictions, y_test_list[[ind]]$class)
  colnames(mc) <- c('predicted', 'truth')
  mc <- as.data.frame(mc)

  mtable <- table(mc$predicted, mc$truth)
  dimnames(mtable) <- list(predicted = c('unbound', 'bound'), truth=c('unbound', 'bound'))

  # plot
  vcd::mosaic(mtable, highlighting = "truth", highlighting_fill = c("pink", "lightblue"), pop=F, main=ind)
  labeling_cells(text = mtable, margin=0)(mtable)
}
```

```{r}

for(ind in individuals){
  lo <- layout(matrix(c(1, 2), nrow=1, ncol=2))

  a <- predict(enet_binary_model, X_test_list[[ind]], type='response') #|> boxplot()
  b <- data.frame(probabilities=a[,1], class=y_test_list[[ind]]$class) #|> as.data.frame()

  boxplot(probabilities ~ class, data=b, col=c('aquamarine', 'darkorange'))
  mtext(glue('Boxplot of distribution of probabilities for {ind}'), line=1, cex=1.5)


  denx <- density(b$probabilities[b$class == 0])
  deny <- density(b$probabilities[b$class == 1])
  #den_unbound <- density(denx)
  plot(denx, ylim = c(0, max(c(denx$y, deny$y))),
      xlim = c(min(c(denx$x, deny$x)),
                max(c(denx$x, deny$x))), xlab='probabilities', main="")
  lines(deny)

  polygon(denx, col = adjustcolor("aquamarine", alpha.f=0.5))
  polygon(deny, col = adjustcolor("darkorange", alpha.f=0.5))
  mtext(glue('Density plot of probabilities for {ind}'), line=1, cex=1.5)
}
```


```{r}

```

```{r}
denx <- density(b$probabilities[b$true_class == 0])
deny <- density(b$probabilities[b$true_class == 1])


#den_unbound <- density(denx)
plot(denx, ylim = c(0, max(c(denx$y, deny$y))),
     xlim = c(min(c(denx$x, deny$x)),
              max(c(denx$x, deny$x))),
              main='Density plot of probabilities', xlab='probabilities')
lines(deny)

polygon(denx, col = adjustcolor("aquamarine", alpha.f=0.5) )
polygon(deny, col = adjustcolor("darkorange", alpha.f=0.5))
```



\newpage
## xgboost models
```{r}
library(xgboost)
library(DiagrammeR)
library(DiagrammeRsvg)
library(rsvg)
```

```{r}
# I should try with a small dataset

# using the aggByCenter 
kawakami_center_dt <- data.table::fread(glue('{kawakami_data_dir}/aggByCenter_{TF}_40000.csv'))

# split the data
X_train <- kawakami_center_dt[, -c(1,2)] |> as.matrix()
y_train <- kawakami_center_dt[, c(1,2)] |> as.data.frame()

# check the distribution
y_train$class |> table()
```

```{r}
# for optimization, I need a test set
set.seed(27102022)

# first bind the data 
dt_all <- cbind(y_train[, 2], X_train) |> as.data.frame()

# split and do your magic
split_dt <- lapply(split(dt_all, f=dt_all$V1), function(each_dt){

  dt_indices <- sample(1:nrow(each_dt), 5000)
  a <- each_dt[dt_indices, ] 
  b <- each_dt[-dt_indices, ]

  return(list(train=a, test=b))

})

train_dt <- rbind(split_dt[[1]][[1]], split_dt[[2]][[1]])
test_dt <- rbind(split_dt[[1]][[2]], split_dt[[2]][[2]])

train_dt <- train_dt[sample(1:nrow(train_dt)), ] |> as.matrix()
test_dt <- test_dt[sample(1:nrow(test_dt)), ] |> as.matrix()

dim(train_dt) ; dim(test_dt)
```

```{r}
# construct a dmatrix
dtrain <- xgboost::xgb.DMatrix(data=train_dt[, -1], label=train_dt[, 1])
dtest <- xgboost::xgb.DMatrix(data=test_dt[, -1], label=test_dt[, 1])
```

```{r, eval=F}
# here we use a grid search to find the optimal hyperparameters
xgb_grid_search <- expand.grid(colsample_bytree = 0.7, max_depth=20, eta=c(0.01, 0.1, 1), gamma=c(5, 10, 20), lambda=c(1, 3, 5), alpha=c(0, 3, 7))
xgb_grid_search
```

```{r, eval=F}
grid_model <- apply(xgb_grid_search, 1, function(each_param){

  params <- list(max_depth=each_param[['max_depth']], eta=each_param[['eta']], colsample_bytree=each_param[['colsample_bytree']],
    gamma=each_param[['gamma']], lambda=each_param[['lambda']], nthreads=8, verbosity=0, objective = "binary:logistic", eval_metric='auc')

  # uses 3-folds cross validation
  xgb_model <- xgboost::xgb.cv(data=dtrain, params=params, nrounds=100, nfold=5, metrics='auc',
    early_stopping_rounds = 20, verbose = 1, print_every_n = 20)

  scores <- as.data.frame(xgb_model$evaluation_log)
  test_auc <- tail(scores$test_auc_mean, 1)
  train_auc <- tail(scores$train_auc_mean, 1)

  return(c(test_auc, train_auc, each_param[['max_depth']], each_param[['eta']], each_param[['colsample_bytree']], each_param[['lambda']], each_param[['gamma']]))

})
```

```{r, eval=F}
grid_result <- t(grid_model) |> as.data.frame()
colnames(grid_result) <- c('test_auc', 'train_auc', 'max_depth', 'learning_rate', 'colsample_bytree', 'lambda', 'gamma')
```


```{r}
#test_boost <- xgboost(dtrain, max.depth = 2, eta = 1, nthread = 2, nrounds = 50, objective = "binary:logistic", eval_metric='auc')

# params <- list(colsample_bytree = 0.7, max_depth=50, eta=c(0.01), gamma=20, lambda=25)
# xgb_model <- xgboost::xgb.cv(data=dtrain, params=params, nrounds=300, nfold=3, metrics='auc',
#     early_stopping_rounds = 10, verbose = 2, print_every_n = 1)
```

```{r}
params <- list(colsample_bytree = 0.7, subsample=0.7, max_depth=30, eta=c(0.1), gamma=25, lambda=3, alpha=3, nthread=8, verbosity=1, objective = "binary:logistic", eval_metric=c('auc', 'aucpr', 'error'))
xgb_model_trained <- xgb.train(data=dtrain, params=params, nrounds=600, watchlist=list(training=dtrain, testing=dtest), print_every_n = 10, early_stopping_rounds = 50)
xgb_model_trained
```

```{r}
plot(xgb_model_trained$evaluation_log$training_auc, type='l', ylim=c(0.7, 1))
lines(xgb_model_trained$evaluation_log$testing_auc, col='red')
```


```{r}
imp_matrix <- xgb.importance(model = xgb_model_trained)
```

```{r}
xgb.plot.importance(importance_matrix = imp_matrix)
```

```{r}
gr <- xgb.plot.tree(model = xgb_model_trained, tree=1, render=F)
export_graph(gr, file_name=glue('{project_dir}/plots/tree_1_node.png'), file_type='png')
```

```{r}
library(SHAPforxgboost)
```

obtain shap values

```{r}
shap_values <- SHAPforxgboost::shap.values(xgb_model_trained, train_dt[, -1])#predict(xgb_model_trained, train_dt[, -1], predcontrib = TRUE, approxcontrib = F)
```



```{r}
save_dir = '/projects/covid-ct/imlab/users/temi/projects/TFXcan/train-test-val/freedman'
ind <- 'LuCaP_145'
test_data_path <- glue('{save_dir}/{ind}_aggByCenter_FOXA1.csv')
val_data <- data.table::fread(test_data_path)

X_val <- val_data[, -c(1:2)] |> as.matrix()
y_val <- val_data[, c(1:2)] |> as.data.frame()
```

```{r}
xgb_prediction <- predict(xgb_model_trained, X_val)
xgb_pred_class <- ifelse(xgb_prediction > 0.5, 1, 0)
```

```{r}
table(xgb_pred_class, y_val[, 2])
```

\newpage

I mostly used this to create a script that I can run on the cluster

```{r}
# here we use a grid search to find the optimal hyperparameters
xgb_grid_search <- expand.grid(colsample_bytree = 0.7, max_depth=20, eta=c(0.01, 0.1, 1), gamma=c(5, 10, 20), lambda=c(1, 3, 5), alpha=c(0, 3, 7))
xgb_grid_search
```

```{r}
# testing for each
#fk_cl <- parallel::makeForkCluster(8)
fk_cl <- parallel::makeCluster(8)
registerDoParallel(fk_cl)

#clusterExport(fk_cl, c('dtrain'))
#clusterEvalQ(fk_cl, 'dtrain')

set.seed(27102022)
#user_matrix <- matrix(sample(1:500, 10*10), nrow=10, ncol=10)
output_cv_result <- foreach(i=1:nrow(xgb_grid_search), .combine='rbind', .packages=c('xgboost'), .inorder=F) %dopar% {

  # this is important - you need to initialize this within the loop !!!
  dtrain <- xgboost::xgb.DMatrix(data=train_dt[, -1], label=train_dt[, 1])

  params <- list(max_depth=xgb_grid_search[i, 'max_depth'], eta=xgb_grid_search[i, 'eta'], colsample_bytree=xgb_grid_search[i, 'colsample_bytree'], gamma=xgb_grid_search[i, 'gamma'], lambda=xgb_grid_search[i, 'lambda'], alpha=xgb_grid_search[i, 'alpha'], nthreads=2, verbosity=0, objective = "binary:logistic", eval_metric='auc', booster='gbtree')

  # # uses 3-folds cross validation
  xgb_model <- xgboost::xgb.cv(data=dtrain, params=params, nrounds=5, nfold=3, metrics='auc', early_stopping_rounds = 20, print_every_n = 1)

  scores <- as.data.frame(xgb_model$evaluation_log)
  test_auc <- tail(scores$test_auc_mean, 1)
  train_auc <- tail(scores$train_auc_mean, 1)

  return(
    c(test_auc, train_auc, 
    xgb_grid_search[i, 'max_depth'], xgb_grid_search[i, 'eta'], 
    xgb_grid_search[i, 'colsample_bytree'], xgb_grid_search[i, 'gamma'], 
    xgb_grid_search[i, 'lambda'], xgb_grid_search[i, 'alpha'])
  )
}

stopCluster(fk_cl)
```








